This chapter goes into the technical details of the Gaussian distribution and
its extension the Gaussian Process (GP). We think it is important to have at
least a basic understanding of the underlying math to make intuitive claims
about the behavior of the model, especially since GPs are a bit different
from other parametric machine learning models.

Since our objective is bayesian optimization, we only derive the properties
necessary for its implementation. Specifically, we are interested in the
conditional and marginal distributions of a multivariate Gaussian. The
conditional Gaussian distribution allows us to compute the posterior $p(f|x)$
at an arbitrary point, and the marginal allows us to fit a GP regression model
to each hyperparameter separately for additional visualization.

Let us now continue with a more rigorous treatment of the Gaussian distribution.
For a more thorough treatment see \cite{bishop2016pattern} and \cite{murphy2012machine}.

\begin{defn}
  A random variable $\rX$ has a \newterm{univariate Gaussian distribution},
  written as $\rX ‚àº ùìù(Œº, œÉ^2)$, when its density is
  $$
    p(x) = \frac{1}{\sqrt{2œÄœÉ¬≤}} \exp{\left\{ -\frac{1}{2œÉ¬≤} (x - Œº)¬≤ \right\}}.
  $$
  The parameters $Œº$ and $œÉ$ are its \emph{mean} and \emph{standard deviation}.
\end{defn}

\begin{defn}
  % TODO: napsat jinak
  We say $\rX$ has a \newterm{degenerate Gaussian distribution} when $\rX ‚àº ùìù(Œº, 0)$.
\end{defn}

% TODO: \mX vs \rX pro vicerozmerne rv
\begin{defn}
  A random variable $\mX \in ‚Ñù^n$ has a \newterm{multivariate Gaussian distribution} if
  any linear combination of its components is a univariate Gaussian, i.e.
  $\va^T \rX = ‚àë_{i=1}^n \va_i \mX_i$ is a Gaussian for all $\va ‚àà ‚Ñù^n$.
  % TODO: tucne Œº a Œ£
  % TODO: cov sequence?
  We then write $\mX ‚àº ùìù(Œº, Œ£)$ where $ùîº[\mX_i] = Œº_i$
  and $cov(\mX_i, \mX_j) = Œ£_{ij}$.
\end{defn}


\begin{rem}
  The parameters $Œº$ and $Œ£$ uniquely determine the distribution $ùìù(Œº, Œ£)$.
\end{rem}

\begin{defn}
  A random variable $\mX ‚àº ùìù(Œº, Œ£)$ has a \newterm{degenerate multivariate Gaussian distribution}
  if $\det Œ£ = \mZero$.
\end{defn}

\begin{rem}
  Given a random variable $\mX ‚àº ùìù(Œº, Œ£)$, random variables $\mX_1, \ldots,
  \mX_n$ are independent with distributions $\mX_i ‚àº ùìù(Œº_i, œÉ_i^2)$ if and only
  if $Œº = (Œº_1, \ldots, Œº_n)$ and $Œ£ = diag(œÉ‚ÇÅ¬≤, \ldots, œÉ‚Çô¬≤)$.
\end{rem}

\begin{thm}
  If a random variable $\mX ‚àà ‚Ñù‚Åø$ is a multivariate Gaussian, then $\rX_i,
  \rX_j$ are independent if and only if $cov(\rX_i, \rX_j) = 0$. Note that his
  is not true for any random variable, as it is a special property of the
  multivariate Gaussian.
\end{thm}

\begin{proof}
  TODO
\end{proof}

\begin{thm}
  A Gaussian random variable $\rX ‚àº ùìù(\vmu, \mSigma)$ has a density iff
  it is non-degenerate (i.e.\ $\det \mSigma \neq 0$, alternatively $\mSigma$
  is positive-definite). And in this case, the density is

  \begin{equation}
    \label{eq:mvn-definition}
    p(\vx) = \frac{1}{\sqrt{\det(2 œÄ \mSigma)}} \exp{ \left\{ - \frac{1}{2}
    (\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu) \right\} }
  \end{equation}
\end{thm}

\begin{rem}
  The normalizing constant in the denominator is also often in an alternate
  form as $$\det(2 œÄ \mSigma) = (2œÄ)^n \det(\mSigma)$$ which follows from basic
  determinant properties. Alternatively we can also put the square root in the
  exponent $(2 \pi)^{n/2} (\det \mSigma)^{1/2}$.
\end{rem}

\begin{rem}
  A special case of the multivariate gaussian is when $n = 1$, then Note that
  if $n = 1$, then $\mSigma = œÉ¬≤$, meaning $cov(X, X) = œÉ¬≤$ , $\mSigma^{-1} =
  \frac{1}{œÉ¬≤}$, and hence the multivariate Gaussian formula becomes the
  univariate one

  \begin{equation}
    p(x) = \frac{1}{\sqrt{2 œÄ œÉ¬≤}} \exp{\left\{ - \frac{1}{2œÉ¬≤} (x - Œº)¬≤ \right\}}.
  \end{equation}
\end{rem}

\section{Sampling}

Even not of immediate interest for bayesian optimization, we will shortly show
how to generate samples from a multivariate Gaussian, as this can be useful for
visualization purposes with GPs.

\begin{thm}
  Given a random variable $\mX$ with $cov[\mX] = \mSigma$, it follows from
  the definition of covariance that $cov[\mA \mX] = \mA \mSigma \mA^T$.
\end{thm}

\begin{proof}
  \begin{align}
    cov[\mA \mX] &= E[(\mA \mX - E[\mA \mX])(\mA \mX - E[\mA \mX])^T] \\
                 &= E[(\mA \mX - \mA E[\mX])(\mA \mX - \mA E[\mX])^T] \\
                 &= E[\mA (\mX - E[\mX])(\mX - E[\mX])^T \mA^T] \\
                 &= \mA E[(\mX - E[\mX])(\mX - E[\mX])^T] \mA^T \\
                 &= \mA cov[\mX] \mA^T \\
                 &= \mA \Sigma \mA^T
    \label{eq:gaussian-ax}
  \end{align}
\end{proof}

\begin{thm}
  Given a random variable $\mX \sim \gN(\mZero, \mI)$ and a positive-definite matrix
  $\mSigma$ with a cholesky decomposition $\mSigma = \mL \mL^T$, then

  \begin{equation}
    \mL \mX \sim \gN(\mZero, \mSigma).
    \label{eq:gaussian-cholesky}
  \end{equation}
\end{thm}

\begin{proof}
  We can immediately use \eqref{eq:gaussian-ax}.
  \begin{align}
    \mL \mX \sim N(0, \mL \mI \mL^T) = N(0, \mL \mL^T) = N(0, \mSigma)
  \end{align}
\end{proof}

\begin{thm}
  Any affine transformation of a Gaussian is a Gaussian. In particular
  $$
    \rX \sim \gN(\vmu, \mSigma) \implies \mA \rX + \vb \sim \gN(\mA \vmu + \vb, \mA \mSigma \mA^T)
  $$
  for any $\vmu \in \mR^n, \mSigma \in \mR^{n \times n}$ positive
  semi-definite, and any $\mA \in \mR^{m \times n}, \vb \in \mR^m$.
  We call this the \newterm{affine property} of a Gaussian.
\end{thm}

\begin{proof}
  Follows from the linearity of expectation together with \autoref{eq:gaussian-cholesky}.
\end{proof}

Since samples from $\gN(\mZero, \mI)$ can be generated independently, using the
affine property we can generate samples from an arbitrary multivariate
Gaussian. All that is required is a procedure for cholesky decomposition, and a
way of generating independent samples from a univariate gaussian, which can be
achieved using the Box-Muller transform \citep{box-muller1958note}.


\section{Geometric Properties}

If $\mSigma$ is positive-definite, then $\mY \sim \gN(\vmu, \mSigma)$ implies
$\mA^{-1} (\mY - \vmu) ‚àº ùìù(0, \mI)$ where $\mSigma = \mA \mA^T$.  The random
variable $\mA^{-1} (\mY - \vmu)$ has a spherical shape in $n$-dimensional
space.

Looking further at the density formula for a multivariate Gaussian
(\autoref{eq:mvn-definition}) the term $(\vx - \vmu)^T \mSigma^{-1}(\vx -
\vmu)$ is called the Mahalanobis distance between $\vx$ and $\vmu$. If we
consider $\vmu$ a constant, we can also view it as a quadratic form in $x$.
When $\mSigma$ is an identity matrix, the Mahalanobis distance reduces to
Euclidean distance. In general, it can be thought of as a distance on a
hyper-ellipsoid. Let us now derive some intuition for this.

Since $\mSigma$ is a covariance matrix, we know it is positive definite, and we
can perform its eigendecomposition to get $\mSigma = \mU \mLambda \mU^T$, where
$\mU$ is an orthogonal matrix of eigenvectors, and $\mLambda$ is a diagonal
matrix of eigenvalues. Basic matrix algebra gives us
$$
  \mSigma^{-1} = (\mU^T)^{-1} \mLambda^{-1} \mU^{-1} = \mU \mLambda^{-1}
  \mU^T = \sum_{i = 1}^D \frac{1}{\lambda_i} \vu_i \vu_i^T,
$$
where the second to last equality comes from $\mU$ being orthogonal ($\mU^{-1}
= \mU^T$).  Substituting this in the Mahalanobis distance we get
\begin{align}
  (\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu) &= (\vx - \vmu)^T \left( \sum_{i = 1}^D \frac{1}{\lambda_i} \vu_i \vu_i^T \right) (\vx - \vmu) \\
                                           &= \sum_{i = 1}^D (\vx - \vmu)^T \frac{1}{\lambda_i} \vu_i \vu_i^T (\vx - \vmu) \\
                                           &= \sum_{i = 1}^D \frac{y_i^2}{\lambda_i} \label{eq:mvn-ellipse}
\end{align}
where $y_i = u_i^T (\vx - \vmu)$ which has exactly the same form as a $D$
dimensional ellipse. From this we conclude that the contour lines of a
multivariate Guassian will be elliptical, where the eigenvectors determine the
orientation of the ellipse, and the eigenvalues determine the length of the
principal axes \citep{bishop2016pattern}.



\section{Conditional and Marginal Gaussian Distribution}

In this section we derive the conditional $p(x_1 | x_2)$ and marginal $p(x_1)$
for a given joint distribution $p(x_1, x_2)$. One of the interesting properties
of a multivariate Gaussian is that both the conditional and the marginal are
also Gaussian, and we can easily compute their parameters in closed from based
on the parameters of the joint distribution.

Before we derive the conditional and marginal distributions, let us state
the partitioned inverse formula without proof.

\begin{thm}[\citep{murphy2012machine}] Consider a partitioned matrix

  \begin{equation}
    \mM = \begin{bmatrix} \mE & \mF \\ \mG & \mH \end{bmatrix}
  \end{equation}

  where we assume $\mE$ and $\mH$ are invertible. We have

  \begin{align}
    \mM^{-1} &= \begin{bmatrix}
      (\mM / \mH)^{-1} & -(\mM / \mH)^{-1} \mF \mH^{-1} \\
      -\mH^{-1} \mG (\mM \mH)^{-1} & \mH^{-1} + \mH^{-1} \mG (\mM / \mH)^{-1} \mF \mH^{-1}
    \end{bmatrix} \\
             &= \begin{bmatrix}
      \mE^{-1} + \mE^{-1} \mF (\mM / \mE)^{-1} \mG \mE^{-1} & - \mE^{-1} \mF (\mM / \mE)^{-1} \\
      -(\mM / \mE)^{-1} \mG \mE^{-1} & (\mM / \mE)^{-1}
    \end{bmatrix}
    \label{eq:matrix-inversion-lemma}
  \end{align}

  where

  \begin{align}
    \mM / \mH &= \mE - \mF \mH^{-1} \mG \\
    \mM / \mE &= \mH - \mG \mE^{-1} \mF
  \end{align}

  is called the \newterm{Schur complement}.
\end{thm}

\begin{proof}
  Since the proof is rather technical and only consists of applying the
  LDU decomposition and many algebraic manipulations, we leave it out and
  refer the reader to \cite{murphy2012machine} for details.
\end{proof}


\subsection{Conditional Distribution is a Gaussian}

Suppose $\vx$ is a $D$-dimensional random vector with a multivariate Gaussian distribution
$\gN(\vx | \vmu, \mSigma)$, and that $\vx$ is partitioned into two vectors
$\vx_1$ and $\vx_2$ such that

\begin{equation}
  \vx = \partx
\end{equation}

We also partition the mean vector $\vmu$ and the covariance matrix $\mSigma$
into a block matrix, and name the inverse of the covariance matrix $\mLambda =
\mSigma^{-1}$, which will simplify a few of the equations that follow. We will
derive the exact form of $\mLambda$ and of its individual blocks later in this
section. For now we simply use the fact that $\mSigma$ is positive-definite,
and thus it is invertible. The matrix $\mLambda$ is also known as a
\newterm{precision matrix}.

\begin{equation}
  \vmu = \partmu,
  \mSigma = \partsigma, \mLambda = \mSigma^{-1} = \partlambda \label{eq:mvn-partition}
\end{equation}

Note that since $\mSigma$ is a symmetric matrix, $\ms{12}^T = \ms{21}$, and
similarly $\ml{12}^T = \ml{21}$. Similarly, $\ms{11}$, $\ms{22}$, $\ml{11}$,
and $\ml{22}$ are all symmetrical.

Before we derive the parameters of the conditional, we show that the
conditional distribution $p(x_1 | x_2)$ is a Gaussian. To do this, we take the
joint distribution $p(x_1, x_2)$ and fix the value of $x_2$
\citep{bishop2016pattern}. Using the definition of conditional probability
$p(x_1, x_2) = p(x_1 | x_2) p(x_2)$ we can see that after fixing the value of
$x_2$, $p(x_2)$ is simply a normalization constant, and the remaining term
$p(x_1 | x_2)$ is a function of $x_1$ which together with the normalization
constant gives us the conditional probability distribution on $x_1$.
We now use the partitioned form of the multivariate Gaussian defined by
\eqref{eq:mvn-partition} to show that $p(x_1 | x_2)$ is actually a Gaussian.

Let us begin by looking at the exponent in \eqref{eq:mvn-definition}:

\begin{align}
  -\frac{1}{2} (\vx - \vmu)^T \mLambda (\vx - \vmu) &=
  -\frac{1}{2} \left(\partx - \partmu \right)^T \partlambda \left(\partx - \partmu \right) \\
                                                    &= -\frac{1}{2} \partxmu^T \partlambda \partxmu
\end{align}

To make the next few equations easier to follow we set $\vy_1 = \vx_1 - \vmu_1$ and $\vy_2 = \vx_2 - \vmu_2$.

\begin{align}
  \begin{split}
    -\frac{1}{2} \begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix} ^T \partlambda \begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix} ={}& -\frac{1}{2} \begin{bmatrix} \vy_1 \ml{11} + \vy_2 \ml{21} \\ \vy_1 \ml{12} + \vy_2 \ml{22} \end{bmatrix} ^T \begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix}
  \end{split} \\
  %
  \begin{split}
    ={}& -\frac{1}{2} \left( \vy_1^T \ml{11} \vy_1 + \vy_2^T \ml{21} \vy_1 + \vy_1^T \ml{12} \vy_2 + \vy_2^T \ml{22} \vy_2 \right)
  \end{split} \\
  %
  \begin{split}
    ={}& -\frac{1}{2} (\vx_1 - \vmu_1)^T \ml{11} (\vx_1 - \vmu_1) {}+ \\
       &-\frac{1}{2} (\vx_2 - \vmu_2)^T \ml{21} (\vx_1 - \vmu_1) {}+ \\
       &-\frac{1}{2} (\vx_1 - \vmu_1)^T \ml{12} (\vx_2 - \vmu_2) {}+ \\
       &-\frac{1}{2} (\vx_2 - \vmu_2)^T \ml{22} (\vx_2 - \vmu_2) \label{eq:mvn-quadratic-form}
  \end{split}
\end{align}

We see that this is a quadratic form in $x_1$, and hence the corresponding
conditional distribution $p(x_1 | x_2)$ will be Gaussian. Because we know
$p(x‚ÇÅ, x‚ÇÇ) = p(x‚ÇÅ|x‚ÇÇ)p(x‚ÇÇ)$ and that both $p(x‚ÇÅ, x‚ÇÇ)$ and $p(x‚ÇÅ|x‚ÇÇ)$ are
multivariate Gaussians, fixing the value of $x‚ÇÅ$ means that $p(x‚ÇÅ|x‚ÇÇ)$ as a
function of $x‚ÇÇ$ is just a normalization constant, and $p(x‚ÇÇ)$ must have the
same form as $p(x‚ÇÅ, x‚ÇÇ)$ and therefore is also a multivariate Gaussian.
\todo{tady ten marginal nevim jiste}

Because the Gaussian distribution is completely defined by its mean and
covariance, we do not need to figure out the value of the normalization
constant. We simply have to derive the equations for $\vmu$ and $\mSigma$.

We continue with the proof from \cite{murphy2012machine}. We will make use of
the partitioned matrix inverse theorem \autoref{eq:matrix-inversion-lemma}.

At this point we know that the joint distribution factors into two multivariate
Gaussians, that is
\begin{align}
  p(x_1, x_2) &= p(x_1 | x_2) p(x_2) \\
              &= \gN(x_1 | \vmu_{1|2}, \ms{1|2}) \gN(x_2 | \vmu_2, \ms{22})
\end{align}
and we only need to infer their parameters.  To make the equations more
readable, we again define

\begin{align}
    \vy_1 &= \vx_1 - \vmu_1 \\
    \vy_2 &= \vx_2 - \vmu_2.
\end{align}

We then simply take the block definition of a multivariate Gaussian and
multiply everything out

\begin{align}
    E &= \exp \left\lbrace -\frac{1}{2}
    \begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix}^T
    \partsigma
    \begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix} \right\rbrace \\
    %
    &= \exp \left\lbrace -\frac{1}{2}
    \begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix}^T
    \begin{bmatrix} \mI & \mZero \\ -\mSigma_{22}^{-1} \mSigma_{21} & \mI \end{bmatrix}
    \begin{bmatrix} (\mSigma/\mSigma_{22})^{-1} & \mZero \\ \mZero & \mSigma_{22}^{-1} \end{bmatrix}
    \begin{bmatrix} \mI & -\mSigma_{12} \mSigma_{22}^{-1} & \\ \mZero & \mI \end{bmatrix}
    \begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix} \right\rbrace \\
    %
    &= \exp \left\lbrace -\frac{1}{2}
    \begin{bmatrix} \vy_1^T - \vy_2^T (\mSigma_{22}^{-1} \mSigma_{21}) \\
    \vy_2
    \end{bmatrix}^T
    \begin{bmatrix} (\mSigma/\mSigma_{22})^{-1} & \mZero \\ \mZero & \mSigma_{22}^{-1} \end{bmatrix} \begin{bmatrix} \vy_1 -\mSigma_{12} \mSigma_{22}^{-1} (\vy_2) \\ \vy_2 \end{bmatrix} \right\rbrace \\
    %
    &= \exp \left\lbrace -\frac{1}{2}
    \begin{bmatrix} (\vy_1^T - \vy_2^T \mSigma_{22}^{-1} \mSigma_{21}) (\mSigma/\mSigma_{22})^{-1} \\
    \vy_2^T \mSigma_{22}^{-1}
    \end{bmatrix}^T
    \begin{bmatrix} \vy_1 -\mSigma_{12} \mSigma_{22}^{-1} (\vy_2) \\ \vy_2 \end{bmatrix}
    \right\rbrace \\
    %
    &= \exp \left\lbrace -\frac{1}{2}
    (\vy_1^T - \vy_2^T \mSigma_{22}^{-1} \mSigma_{21}) (\mSigma/\mSigma_{22})^{-1} (\vy_1 -\mSigma_{12} \mSigma_{22}^{-1} \vy_2)
    \right\rbrace \times \\
    & \qquad\qquad \times \exp \left\lbrace -\frac{1}{2} \vy_2^T \mSigma_{22}^{-1} \vy_2 \right\rbrace \nonumber
\end{align}

We can immediately see that the second term is a quadratic form in $\vx_2$ and
corresponds to $\gN(\vx_2 | \vmu_2, \mSigma_{22})$. Let us now consider the
first term in isolation and move the terms around a little bit. We also make
use of the fact that because $\mSigma_{22}$ is a positive-definite matrix, its
inverse is also symmetric, so $\mSigma^{-1^T}_{22} = \mSigma^{-1}_{22}$. We
also know that $\mSigma^T_{12} = \mSigma_{21}$.

\begin{align}
    E_{1|2} &= \exp \left\lbrace -\frac{1}{2}
    (\vy_1^T - \vy_2^T \mSigma_{22}^{-1} \mSigma_{21}) (\mSigma/\mSigma_{22})^{-1} (\vy_1 -\mSigma_{12} \mSigma_{22}^{-1} \vy_2) \right\rbrace \\
    &= \exp \left\lbrace -\frac{1}{2}
    (\vy_1 - \mSigma_{12}\mSigma_{22}^{-1} \vy_2)^T (\mSigma/\mSigma_{22})^{-1} (\vy_1 -\mSigma_{12} \mSigma_{22}^{-1} \vy_2) \right\rbrace \\
    &= \exp \{ -\frac{1}{2}
    (\vx_1 - \vmu_1 - \mSigma_{12}\mSigma_{22}^{-1} (\vx_2 - \vmu_2)^T (\mSigma/\mSigma_{22})^{-1} \\
    & \hspace{40pt} (\vx_1 - \vmu_1 -\mSigma_{12} \mSigma_{22}^{-1} (\vx_2 - \vmu_2)) \}
    \label{eq:mvn-quadratic-form-12}
\end{align}

In \eqref{eq:mvn-quadratic-form-12} we again see a Gaussian density with
parameters
\begin{align}
  \vmu_{1|2} &= \vmu_1 + \mSigma_{12}\mSigma_{22}^{-1} (\vx_2 - \vmu_2) \\
  \mSigma_{1|2} &= (\mSigma/\mSigma_{22})^{-1} =  \ms{11} - \ms{12} \ms{22}^{-1} \ms{21}.
\end{align}
This formula extremely important for the use of GPs as a probabilistic
model in bayesian optimization. It will allow us to compute the exact
parameters of the posterior $p(f|x)$ at any given point, and as a result
compute the acquisition function.


\section{Gaussian Processes}

\newterm{Gaussian Process} is a stochastic process (a collection of random
variables), such that every subset of those random variables has a
multivariate Gaussian distribution. It is defined by a mean function
$m(\vx)$ and a covariance funciton $\kappa(\vx, \vx')$. Formally, we write
\begin{equation}
  p(\vx) \sim ùìñùìü (m(\vx), \kappa(\vx, \vx')).
\end{equation}
Any finite subset $\vx = (x_1, \ldots, x_n)$ is jointly Gaussian with mean
$m(\vx)$ and covariance $\mSigma(\vx)$ where $\mSigma(\vx)_{ij} =
\kappa(\vx_i, \vx_j)$, where $\kappa$ is any positive definite kernel
function \citep{murphy2012machine}.

The GP defines a prior distribution over functions $f$, which when
combined with data $x$ can be converted into a posterior distribution
over functions $p(f|x)$.

\subsection{GP regression with noise-free observations}

Consider the case when we are interested in predicting a function $f$
based on a few observations $ùìì = \{(\vx_1, y_1), \ldots, (\vx_n, y_n)\}$,
and we are interested in predicting the value of $y_\star$ at a new point
$x_\star$.

Using the definition of a GP, we know that $y$ and $y_\star$ are jointly
Gaussian. We also know, that these are the only points we are interested
in.  Even though the GP is a distribution over functions, that is over
infintiely dimensional vectors, we only need to look at finitely many
points and can ignore the rest.  This is a crucial property of the GP and
essentially makes everything we are about to do possible.

By assuming a GP, we get all of the properties of a multivariate Gaussian
for free, including a closed form solution to the conditional and marginal
distribution parameters. Let us now write the joint distribution of $y$ and $y_\star$ in
a partitioned form
$$
  \begin{pmatrix} y \\ y_\star \end{pmatrix} ‚àº ùìù\begin{pmatrix}
  \begin{pmatrix} \vmu \\ \vmu_\star \end{pmatrix},
  \begin{pmatrix} \mK & \mK_\star \\ \mK_\star^T & \mK_{\star\star} \end{pmatrix}
  \end{pmatrix}
$$
where $\mK = \kappa(\mX, \mX)$, $\mK_\star = \kappa(\mX, \mX_\star)$ and
$\mK_{\star\star} = \kappa(\mX_\star, \mX_\star)$ \citep{williams2006gaussian}.
Because this is just a multivariate Gaussian, we can make use of the conditioning
formula and compute the posterior $p(y_\star | \mX_\star, \mX, y)$ exactly
as
\begin{align}
  p(y_\star | \mX_\star, \mX, y) &= ùìù(y_\star | \vmu_\star, \mSigma_\star)
  \\
  \vmu_\star &= \vmu(\mX_\star) + \mK_\star^T \mK^{-1} (y - \vmu(\mX))
  \\
  \mSigma_\star &= \mK_{\star\star} - \mK_\star^T \mK^{-1} \mK_\star.
\end{align}.

\subsection{GP regression with noisy observations}

TODO
