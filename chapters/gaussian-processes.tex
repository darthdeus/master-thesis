This chapter goes into the technical details of the Gaussian distribution and
its extension the Gaussian Process (GP). We think it is important to have at
least a basic understanding of the underlying math to make intuitive claims
about the behavior of the model, especially since GPs are a bit different
from other parametric machine learning models.

Since our objective is bayesian optimization, we only derive the properties
necessary for its implementation. Specifically, we are interested in the
conditional and marginal distributions of a multivariate Gaussian. The
conditional Gaussian distribution allows us to compute the posterior $p(f|x)$
at an arbitrary point, and the marginal allows us to fit a GP regression model
to each hyperparameter separately for additional visualization.

Let us now continue with a more rigorous treatment of the Gaussian distribution.
For a more thorough treatment see \cite{bishop2016pattern} and \cite{murphy2012machine}.

\begin{defn}
  A random variable $\rX$ has a \newterm{univariate Gaussian distribution},
  written as $\rX âˆ¼ ğ“(Î¼, Ïƒ^2)$, when its density is
  $$
    p(x) = \frac{1}{\sqrt{2Ï€ÏƒÂ²}} \exp{\left\{ -\frac{1}{2ÏƒÂ²} (x - Î¼)Â² \right\}}.
  $$
  The parameters $Î¼$ and $Ïƒ$ are its \emph{mean} and \emph{standard deviation}.
\end{defn}

\begin{defn}
  % TODO: napsat jinak
  We say $\rX$ has a \newterm{degenerate Gaussian distribution} when $\rX âˆ¼ ğ“(Î¼, 0)$.
\end{defn}

% TODO: \mX vs \rX pro vicerozmerne rv
\begin{defn}
  A random variable $\mX \in â„^n$ has a \newterm{multivariate Gaussian distribution} if
  any linear combination of its components is a univariate Gaussian, i.e.
  $\va^T \rX = âˆ‘_{i=1}^n \va_i \mX_i$ is a Gaussian for all $\va âˆˆ â„^n$.
  % TODO: tucne Î¼ a Î£
  % TODO: cov sequence?
  We then write $\mX âˆ¼ ğ“(Î¼, Î£)$ where $ğ”¼[\mX_i] = Î¼_i$
  and $cov(\mX_i, \mX_j) = Î£_{ij}$.
\end{defn}


\begin{rem}
  The parameters $Î¼$ and $Î£$ uniquely determine the distribution $ğ“(Î¼, Î£)$.
\end{rem}

\begin{defn}
  A random variable $\mX âˆ¼ ğ“(Î¼, Î£)$ has a \newterm{degenerate multivariate Gaussian distribution}
  if $\det Î£ = \mZero$.
\end{defn}

\begin{rem}
  Given a random variable $\mX âˆ¼ ğ“(Î¼, Î£)$, random variables $\mX_1, \ldots,
  \mX_n$ are independent with distributions $\mX_i âˆ¼ ğ“(Î¼_i, Ïƒ_i^2)$ if and only
  if $Î¼ = (Î¼_1, \ldots, Î¼_n)$ and $Î£ = diag(Ïƒâ‚Â², \ldots, Ïƒâ‚™Â²)$.
\end{rem}

\begin{thm}
  If a random variable $\mX âˆˆ â„â¿$ is a multivariate Gaussian, then $\rX_i,
  \rX_j$ are independent if and only if $cov(\rX_i, \rX_j) = 0$. Note that his
  is not true for any random variable, as it is a special property of the
  multivariate Gaussian.
\end{thm}

\begin{proof}
  TODO
\end{proof}

\begin{thm}
  A Gaussian random variable $\rX âˆ¼ ğ“(\vmu, \mSigma)$ has a density iff
  it is non-degenerate (i.e.\ $\det \mSigma \neq 0$, alternatively $\mSigma$
  is positive-definite). And in this case, the density is

  \begin{equation}
    \label{eq:mvn-definition}
    p(\vx) = \frac{1}{\sqrt{\det(2 Ï€ \mSigma)}} \exp{ \left\{ - \frac{1}{2}
    (\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu) \right\} }
  \end{equation}
\end{thm}

\begin{rem}
  The normalizing constant in the denominator is also often in an alternate
  form as $$\det(2 Ï€ \mSigma) = (2Ï€)^n \det(\mSigma)$$ which follows from basic
  determinant properties. Alternatively we can also put the square root in the
  exponent $(2 \pi)^{n/2} (\det \mSigma)^{1/2}$.
\end{rem}

\begin{rem}
  A special case of the multivariate gaussian is when $n = 1$, then Note that
  if $n = 1$, then $\mSigma = ÏƒÂ²$, meaning $cov(X, X) = ÏƒÂ²$ , $\mSigma^{-1} =
  \frac{1}{ÏƒÂ²}$, and hence the multivariate Gaussian formula becomes the
  univariate one

  \begin{equation}
    p(x) = \frac{1}{\sqrt{2 Ï€ ÏƒÂ²}} \exp{\left\{ - \frac{1}{2ÏƒÂ²} (x - Î¼)Â² \right\}}.
  \end{equation}
\end{rem}

\section{Sampling}

Even not of immediate interest for bayesian optimization, we will shortly show
how to generate samples from a multivariate Gaussian, as this can be useful for
visualization purposes with GPs.

\begin{thm}
  Given a random variable $\mX$ with $cov[\mX] = \mSigma$, it follows from
  the definition of covariance that $cov[\mA \mX] = \mA \mSigma \mA^T$.
\end{thm}

\begin{proof}
  \begin{align}
    cov[\mA \mX] &= E[(\mA \mX - E[\mA \mX])(\mA \mX - E[\mA \mX])^T] \\
                 &= E[(\mA \mX - \mA E[\mX])(\mA \mX - \mA E[\mX])^T] \\
                 &= E[\mA (\mX - E[\mX])(\mX - E[\mX])^T \mA^T] \\
                 &= \mA E[(\mX - E[\mX])(\mX - E[\mX])^T] \mA^T \\
                 &= \mA cov[\mX] \mA^T \\
                 &= \mA \Sigma \mA^T
    \label{eq:gaussian-ax}
  \end{align}
\end{proof}

\begin{thm}
  Given a random variable $\mX \sim \gN(\mZero, \mI)$ and a positive-definite matrix
  $\mSigma$ with a cholesky decomposition $\mSigma = \mL \mL^T$, then

  \begin{equation}
    \mL \mX \sim \gN(\mZero, \mSigma).
    \label{eq:gaussian-cholesky}
  \end{equation}
\end{thm}

\begin{proof}
  We can immediately use \eqref{eq:gaussian-ax}.
  \begin{align}
    \mL \mX \sim N(0, \mL \mI \mL^T) = N(0, \mL \mL^T) = N(0, \mSigma)
  \end{align}
\end{proof}

\begin{thm}
  Any affine transformation of a Gaussian is a Gaussian. In particular
  $$
    \rX \sim \gN(\vmu, \mSigma) \implies \mA \rX + \vb \sim \gN(\mA \vmu + \vb, \mA \mSigma \mA^T)
  $$
  for any $\vmu \in \mR^n, \mSigma \in \mR^{n \times n}$ positive
  semi-definite, and any $\mA \in \mR^{m \times n}, \vb \in \mR^m$.
  We call this the \newterm{affine property} of a Gaussian.
\end{thm}

\begin{proof}
  Follows from the linearity of expectation together with \autoref{eq:gaussian-cholesky}.
\end{proof}

Since samples from $\gN(\mZero, \mI)$ can be generated independently, using the
affine property we can generate samples from an arbitrary multivariate
Gaussian. All that is required is a procedure for cholesky decomposition, and a
way of generating independent samples from a univariate gaussian, which can be
achieved using the Box-Muller transform \citep{box-muller1958note}.


\section{Geometric Properties}

If $\mSigma$ is positive-definite, then $\mY \sim \gN(\vmu, \mSigma)$ implies
$\mA^{-1} (\mY - \vmu) âˆ¼ ğ“(0, \mI)$ where $\mSigma = \mA \mA^T$.  The random
variable $\mA^{-1} (\mY - \vmu)$ has a spherical shape in $n$-dimensional
space.

Looking further at the density formula for a multivariate Gaussian
(\autoref{eq:mvn-definition}) the term $(\vx - \vmu)^T \mSigma^{-1}(\vx -
\vmu)$ is called the Mahalanobis distance between $\vx$ and $\vmu$. If we
consider $\vmu$ a constant, we can also view it as a quadratic form in $x$.
When $\mSigma$ is an identity matrix, the Mahalanobis distance reduces to
Euclidean distance. In general, it can be thought of as a distance on a
hyper-ellipsoid. Let us now derive some intuition for this.

Since $\mSigma$ is a covariance matrix, we know it is positive definite, and we
can perform its eigendecomposition to get $\mSigma = \mU \mLambda \mU^T$, where
$\mU$ is an orthogonal matrix of eigenvectors, and $\mLambda$ is a diagonal
matrix of eigenvalues. Basic matrix algebra gives us
$$
  \mSigma^{-1} = (\mU^T)^{-1} \mLambda^{-1} \mU^{-1} = \mU \mLambda^{-1}
  \mU^T = \sum_{i = 1}^D \frac{1}{\lambda_i} \vu_i \vu_i^T,
$$
where the second to last equality comes from $\mU$ being orthogonal ($\mU^{-1}
= \mU^T$).  Substituting this in the Mahalanobis distance we get
\begin{align}
  (\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu) &= (\vx - \vmu)^T \left( \sum_{i = 1}^D \frac{1}{\lambda_i} \vu_i \vu_i^T \right) (\vx - \vmu) \\
                                           &= \sum_{i = 1}^D (\vx - \vmu)^T \frac{1}{\lambda_i} \vu_i \vu_i^T (\vx - \vmu) \\
                                           &= \sum_{i = 1}^D \frac{y_i^2}{\lambda_i} \label{eq:mvn-ellipse}
\end{align}
where $y_i = u_i^T (\vx - \vmu)$ which has exactly the same form as a $D$
dimensional ellipse. From this we conclude that the contour lines of a
multivariate Guassian will be elliptical, where the eigenvectors determine the
orientation of the ellipse, and the eigenvalues determine the length of the
principal axes \citep{bishop2016pattern}.


