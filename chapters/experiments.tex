\chapter{Experiments}
\label{chapter:experiments}

This chapter presents experiments we have conducted to evaluate the viability of Bayesian optimization. All but the first of the experiments are examples of optimizing neural networks, because that is the area of interest of this work.

In the first experiment we chose the Eggholder function \citep{eggholder}, commonly used to measure the performance of optimizers, and use it as an additional baseline for comparison with random search. The Eggholder function is an interesting optimization target, because it has only one global minimum, but many local minima with an unpredictable landscape.

Next we present one experiment from reinforcement learning, specifically the REINFORCE algorithm \citep{suttonbarto2018reinforcement}, as an example of a smaller and simpler network. In this setting, we also compare Bayesian optimization with a random search. Next we show a larger experiment where we train a recurrent neural network \citep{dlbook}, specifically a tagger, on a SIGMORPHON 2019 shared task \citep{sigmorphon2019task2}, where the network is trained on a significantly larger dataset in multiple languages. Lastly, we show two more experiments, one of a tokenizer written in C++, and second a network for speech recognition.

\section{The Eggholder Function}

As our first experiment, we evaluate the Eggholder function \cite{eggholder} due to its popularity as a test function for evaluating optimizer performance. A visualization of the Eggholder function is presented in \autoref{figure:eggholder}.

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.6\textwidth]{images/eggholder.png}
		% JA: Ten text je trochu divny, ale nepovedlo se mi jinak zaridit, aby ta definice byla na dalsi radce a nevypadalo to divne.
		\caption{The Eggholder objective function defined on $(x, y) \in \mathrm{R}^2$ analytically as $f(x, y) = -(y + 47) \sin \sqrt{| \frac{x}{2} + (y + 47)|} - x \sin \sqrt{| x - (y + 47)|} $. It has one global minimum at $(512, 404.23)$ with $y = -959.64$. When optimizing the function we instead optimize $-f(x,y)$ as our implementation always maximizes the objective.}
		\label{figure:eggholder}
	\end{center}
\end{figure}

We ran $50$ repetitions of the experiment with Bayesian optimization and random search each, and display the results in \autoref{figure:eggholder-comparison}. The average rank on the total $100$ experiments is $25.5$ for Bayesian optimization, and $75.5$ for random search.

While a toy example, this experiment shows a first evidence of Bayesian optimization outperforming random search when optimizing a non-convex function.

\begin{figure}
	\begin{center}
		\includegraphics[width=0.6\textwidth]{images/eggholder-comparison.png}
		\caption{A plot comparing the results of Bayesian optimization (blue) and random search (red) when optimizing the Eggholder function.}
		\label{figure:eggholder-comparison}
	\end{center}
\end{figure}

\section{REINFORCE}
\label{section:experiments-empirical-bayes}

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/cartpole.png}
		\caption{The objective of the task is to balance the pole on top of a cart moving along
			a track. A failure occurs if the pole falls beyond a certain angle, and the agent
			is rewarded $+1$ for every time step when the pole has not fallen over.}
		\label{figure:cartpole}
	\end{center}
\end{figure}

In order to evaluate the effectiveness of Bayesian optimization and compare it to random search, we chose the REINFORCE algorithm on a simple task in reinforcement learning, because the network is small and fast to train, but at the same time sensitive to hyperparameters. The goal of the agent is to balance a pole on top of a cart by moving either left or right, as shown in \autoref{figure:cartpole}. For each time step when the pole has not fallen over the agent receives $+1$ reward, up to a maximum of $500$. Because an episode can end rather quickly when the agent performs a few bad actions, some combinations of hyperparameters results in very low values of the objective function (e.g., $< 50$). On the other hand, reaching the perfect $500$ reward is not very difficult after some exploration of the search space. Out of our $60$ different runs of the experiment, only one was not able to find the perfect score in the allotted $50$ evaluations of the objective function.

Overall, we repeated the experiment three times. Once purely with a random search, once with a non-informative prior, and once with an informative prior, where the mean was set to the mean of the hyperparameter range (per hyperparameter) and standard deviation to a quarter of the range. Each of these setups was then executed $20$ times, each with $50$ evaluations of the objective function. Because of the noisy aspect of reinforcement learning, we modified the objective function so that instead of training and evaluating the network once, we trained it yet another $20$ times from scratch, evaluated each separately, and reported the average objective achieved. Therefore, if the optimizer receives an objective value of $500$, then all $20$ of the sub-evaluations managed to reach that score.

We show the numeric results in \autoref{table:reinforce-results}. The \emph{AUC} column measures the area under the convergence plot (see \autoref{figure:convergence-plot}), specifically the area under the cumulative maximum. The value was then normalized so that it would be $1$ exactly when a score of $500$ was achieved on the first sample. We also report the mean $\mu_{\text{time to max}}$ and standard deviation $\sigma_{\text{time to max}}$ of the time (number of evaluations) it take the optimizer to achieve the score of $500$.

\begin{table}[t]
	\begin{center}
		\begin{tabular}{||c c c c||} 
			\hline
			Optimizer & AUC & $\mu_{\text{time to max}}$ & $\sigma_{\text{time to max}}$ \\ [0.5ex] 
			\hline\hline
			Random Search & $0.89$ & $19.35$ & $15.73$ \\ 
			\hline
			Non-informative Prior & $0.91$ & $12.75$ & $10.73$ \\
			\hline
			Informative Prior & $0.90$ & $12.25$ & $9.74$  \\
			\hline
		\end{tabular}
		\caption{Results of a random search and Bayesian optimization with a non-informative and informative priors on the cart-pole balancing task.}
		\label{table:reinforce-results}
	\end{center}
\end{table}

Intuitively, the $\mu_{\text{time to max}}$ shows how fast the optimizer is able to find the maximum of the objective function, and $\sigma_{\text{time to max}}$ measures its consistency. The results clearly show that Bayesian optimization as compared to a random search is almost twice as fast at finding the maximum, and also achieves lower variance.

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/informative-priors.png}
		\caption{Kernel parameters over time in the case of informative priors, where the prior is set based on the hyperparameter range. Parameters almost never rapidly change their value after the initial few samples.}
		\label{figure:informative-priors}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/reinforce-jumpy-kernel-params.png}
		\caption{Kernel parameters over time in the case of non-informative priors, where the prior does not depend on the hyperparameter range. Some hyperparameters do not stabilize over time and their scale changes by orders of magnitude.}
		\label{figure:reinforce-jumpy-kernel-params}
	\end{center}
\end{figure}


The informative prior also achieves slightly lower variance than the non-informative one, which matches our intuition of guiding the GP regression model into a more stable area of the kernel parameter space. We confirm this with a kernel parameter visualization as shown in \autoref{figure:informative-priors}, where we can clearly see the parameters to be more stable, than in the case of non-informative priors as shown in \autoref{figure:reinforce-jumpy-kernel-params}.

As a side note, this experiment provided an interesting technical dilemma. Evaluating noisy objective functions is an interesting problem from the practical perspective. Even though the GP regression model is able to model noise in the objective function directly, it can only do so after it is given enough data points which show the noisy trend. But in the context of Bayesian optimization we need to use the model to make decisions about the next evaluation point, even when the number of data points is low. We could run a larger number of evaluations using random search initially, so that the GP could fit the noise in the data. Because of some of the problems with overfitting shown earlier in \autoref{chapter:bo-indepth}, we chose not to model the noise with the model directly, because that could lead to more potential failure points. Instead, we took a practical approach and simply reduced the noise by performing multiple (specifically $20$) evaluations of the REINFORCE model for each set of hyperparameters, and averaged the result. Since this method is a Monte Carlo approximation of the true objective function, its variance decreases as such (roughly as $\frac{1}{\sqrt{N}}$), the GP will be able to fit the objective function with almost an order of magnitude lower noise parameter.


\section{Tagger on a Single Treebank}
\label{section:exp-cestina}

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/czech-2d-marginal.png}
		\caption{Marginal regression on the \emph{label\_smoothing} and \emph{dropout} hyperparameters for the SIGMORPHON 2019 shared task when training on a single Czech treebank. A correlation between the two parameters is clearly visible, showing that setting only one of them to the correct value is not enough to achieve a high value of the objective function.}
		\label{figure:czech-2d-marginal}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/czech-convergence.png}
		\caption{Convergence plot showing the process of Bayesian optimization with a cumulative maximum shown in orange.}
		\label{figure:czech-convergence}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/czech-hyperparam-trends.png}
		\caption{Marginal regression on the \emph{batch\_size}, \emph{dropout} and $\beta_2$ hyperparameters for the SIGMORPHON 2019 shared task when training on a single Czech treebank. All three cases show a visible increasing trend.}
		\label{figure:czech-batch-size}
	\end{center}
\end{figure}


Contrary to the experiment shown in \autoref{section:multiple-treebanks-failed} in this experiment we train the same model but only on a single treebank at a time. We tried one of our largest treebanks, specifically for the Czech language. We chose a larger treebank because it provides a more consistent evaluation metric, with the smaller ones being several orders of magnitude smaller, which may result in overfitting even on the validation set.

While the next experiment (see \autoref{section:multiple-treebanks-failed}) did not find any trends in the hyperparameters, the one we present here did find a clear trend in most, as shown for example in \autoref{figure:czech-batch-size}, where larger \emph{batch\_size} is better. In some cases we also found clear relationships between two hyperparameters, such as in the case of \emph{label\_smoothing} and \emph{dropout} shown in \autoref{figure:czech-2d-marginal}.

The experiment used five parallel evaluations, the first of which were seeded with random search. The baseline model achieved $97.4\%$ accuracy, while the random search achieved only $97.32\%$, and the remaining search with Bayesian optimization surpassed the baseline with final $97.44\%$ (see \autoref{figure:czech-convergence}).


\section{Tuning on Multiple Treebanks Simultaneously}
\label{section:multiple-treebanks-failed}

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/sig-convergence.png}
		\caption{Convergence plot for a model trained on the SIGMORPHON 2019 shared task. No significant improvement over the baseline was found when training on all 105 treebanks simultaneously.}
		\label{figure:sigmorphon-convergence}
	\end{center}
\end{figure}

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/sig-dropout.png}
		\caption{Marginal regression on the \emph{dropout} hyperparameter for the SIGMORPHON 2019 shared task when training on all 105 treebanks.}
		\label{figure:sigmorphon-dropout}
	\end{center}
\end{figure}

In this larger experiment we tried to tune hyperparameters for a single model on multiple different datasets at once. Specifically, the model was trained for the SIGMORPHON 2019 shared task 2 \citep{sigmorphon2019task2} on 105 different treebanks, most of which are different languages.\footnote{The shared task training data consists of 107 treebanks, but we did not consider the two largest in the Bayesian optimization for performance reasons.} Our goal was to ideally find one set of hyperparameters that would work well across all of these treebanks. Even though this experiment was not ultimately successful, we still include it in the text as the problem being solved poses interesting technical challenges from the perspective of hyperparameter tuning.

As a fundamental problem of this task, each treebank is of different size, and the difficulty of different languages varies a lot. To give a few specific examples, on Tamil the model achieves around 95\% accuracy in tagging, while on Sanskrit it achieves only 65\%. This makes it nearly impossible to set the objective function to simply be the accuracy of the model, as the same value of hyperparameters could result in the objective varying by 30\%.

We work around this problem by computing a baseline accuracy using an existing, hand tuned, model on the same task, pre-computing its score for each treebank with a fixed set of hyperparameters, and then subtracting its value from the accuracy of our tuned model. The objective function then explicitly becomes the improvement over an existing fixed model.

A second problem we had to solve was how to incorporate the 105 treebanks into the optimization procedure. Because training the model takes multiple GPU hours even for the smaller treebanks, we realistically cannot perform more than a few hundred evaluations total. Instead of training the model on all treebanks for each set of hyperparameters, we only train the model on one treebank for each hyperparameter configuration chosen by Bayesian optimization.

This introduces an interesting choice. In theory, we could treat the treebank as a hyperparameter treated explicitly by Bayesian optimization. As the algorithm balances the exploration-exploitation trade-off, it should be able to pick treebanks as needed to better explore the space. But because of the different sizes and difficulties of each treebank, and the inability of Bayesian optimization to treat categorical hyperparameters well, we decided to not take this approach.

Instead, we omit the treebank from the list of hyperparameters, so that the optimizer has no way of modeling it, and provide it as an explicit parameter outside of the scope of the optimizer. In theory, we could also provide the optimizer with a treebank that was used with each evaluation, not allowing it to optimize the treebank when choosing a next point by overriding its choice, but this approach would most likely yield sub-optimal choices by the optimizer, as the samples would be chosen based on a different criteria than what the optimizer considers. Instead, our way of completely removing the treebank parameter from the optimization process will be seen by the optimizer as noise on the output. If it were to evaluate the same hyperparameters multiple times, the evaluation itself would receive a different treebank, and the improvement over the baseline would be different. In effect this captures our desire to find a unique set of hyperparameters that works well across all treebanks.

Unfortunately, our experimental results did not find a significant improvement over the hyperparameters found by hand-tuning the network (see \autoref{figure:sigmorphon-convergence}). One of the reasons is that the size of the treebank affects which hyperparameter combinations result in value of the objective function. An example of using tuning the same model on a larger treebank is shown in \autoref{section:exp-cestina}.

To give a specific example, when the \emph{batch\_size} hyperparameter is set to a larger value, it works well on a larger treebank, but the opposite is true for a smaller treebank, where a smaller \emph{batch\_size} acts as a regularizer. This means our samples at the same batch size will be spread as far as is the objective on all treebanks. Unfortunately, this spread ends up being so large that all of the hyperparameter end up having no visible trend, as no value is clearly better than the others, as shown in \autoref{figure:sigmorphon-dropout}.

We tried splitting the treebanks into two groups, one with the largest $25\%$ and one with the smallest $25\%$ (based on number of words in the treebank). Then we ran the same optimization procedure on the same hyperparameters, and confirmed our earlier theory about smaller \emph{batch\_size} working better for smaller treebanks and vice versa. \autoref{figure:sig-small-batch} shows the marginal plot on the smaller treebanks, while \autoref{figure:sig-large-batch} shows the same on larger treebanks.

While we did not find any improvement over the baseline on the larger treebanks, the smaller treebanks yielded a $3.56\%$ increase over the baseline on the Akkadian treebank. Unfortunately, that is the only treebank on which we managed to improve over the baseline by more than $0.1\%$. This experiment shows how noise can negatively affect the underlying probabilistic model, since training on multiple larger treebanks did not yield any improvement, but training on a larger treebank in isolation in the previous experiment did (see \autoref{section:exp-cestina}).

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/sig-small-batch.png}
		\caption{Marginal regression on the \emph{batch\_size} hyperparameter when trained on the smallest $25\%$ treebanks.}
		\label{figure:sig-small-batch}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/sig-large-batch.png}
		\caption{Marginal regression on the \emph{batch\_size} hyperparameter when trained on the largest $25\%$ treebanks.}
		\label{figure:sig-large-batch}
	\end{center}
\end{figure}

We also compare the convergence plots on both the smaller (see \autoref{figure:sig-small-convergence}) and larger (see \autoref{figure:sig-large-convergence}) subsets of treebanks. The size difference between the largest and smallest treebank is over three orders of magnitude, specifically $1,207,922$ words for Czech and $230$ words for Tagalog.


\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/sig-small-convergence.png}
		\caption{Convergence plot when training on the smallest $25\%$ treebanks, showing maximum improvement of $4\%$ accuracy over the baseline model.}
		\label{figure:sig-small-convergence}
	\end{center}
\end{figure}


\begin{figure}
	\begin{center}
		\includegraphics[width=1\textwidth]{images/sig-large-convergence.png}
		\caption{Convergence plot when training on the largest $25\%$ treebanks with no improvement over the baseline model.}
		\label{figure:sig-large-convergence}
	\end{center}
\end{figure}


\begin{figure}
	\begin{center}
		\includegraphics[width=1\textwidth]{images/tokenizer-overfitting-batch-size.png}
		\caption{GP regression overfitting on the marginal distribution of \emph{batch\_size}.}
		\label{figure:gp-overfitting-batch-size}
	\end{center}
\end{figure}

\clearpage
\section{Segmentation}

In this experiment we optimize hyperparameters of UDPipe \citep{udpipe:2017} on tokenization of Czech.

Some of the interesting result in this case are the failure cases of the GP regression, particularly how it can overfit on seemingly uninformative dataset, as shown in \autoref{figure:gp-overfitting-batch-size}. These examples nicely show the high capacity of a GP, which, even when we set informative priors, is able to find a surprising way to fit the data with high likelihood. Despite these problems, the Bayesian optimization approach still managed to find hyperparameter values with an objective very close to that find by an exhaustive grid search. Because of the acquisition function, even if the model overfits and picks a bad sample, it will be able to correct itself right afterwards, as the area that was previously receiving high value of acquisition function was explored.

As a result, the model still might end up wasting computation time by evaluating incorrect areas by overfitting on the data, but we have not observed it to get stuck in a local optima. Because Bayesian optimization explicitly models exploration into its decision making it quite often ends up doing a decent job of exploring most of the search space, as is visible in all of the figures shown in this chapter.

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/tokenizer-convergence.png}
		\caption{Convergence plot for the segmentation experiment showing the progress of Bayesian optimization.}
		\label{figure:tokenizer-convergence}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/tokenizer-comparison.png}
		\caption{Comparison between Bayesian optimization (blue) and random search (red) on $10$ different runs of the segmentation experiment.}
		\label{figure:tokenizer-comparison}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=1.0\textwidth]{images/tokenizer-bopt-vs-grid.png}
		\caption{Comparison between Bayesian optimization (blue) and grid search (red) on $10$ different runs of Bayesian optimization versus $72$ evaluations of grid search.}
		\label{figure:tokenizer-comparison-bopt-grid-search}
	\end{center}
\end{figure}

In this case the result tuned with Bayesian optimization did not surpass the hand-tuned baseline, although an upward trend is visible in the convergence plot shown in \autoref{figure:tokenizer-convergence}. We intentionally limited the number of evaluations to $50$ to make comparisons with the hand-tuned baselines, because the goal of Bayesian optimization is to find the optimum quickly.

We compare our achieved accuracy over $10$ runs of Bayesian optimization with $10$ runs of random search in \autoref{figure:tokenizer-comparison}. The average rank is $5.5$ for Bayesian optimization, and $15.5$ for random search. In \autoref{figure:tokenizer-comparison-bopt-grid-search} we also compare the same $10$ runs of Bayesian optimization to $72$ evaluations of grid search. Only one of the grid search results managed to achieve a superior value of the objective function, with the remaining values being significantly worse. The overall rank is $5.5$ for Bayesian optimization, and $46.5$ for grid search (out of total $82$ evaluations).

Despite grid search achieving a maximum in one sample, we believe the overall ranking shows how Bayesian optimization consistently out-performs grid search using fewer evaluations.

\section{Speech Recognition}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\textwidth]{images/speech-2d-almost-insensitive.png}
		\caption{Marginal 2D regression on the \emph{high\_freq} and \emph{input\_dropout} hyperparameters. The range of the hyperparameters was coincidentally chosen so that only values at the very edge end up with a low value of the objective function, and are indifferent in most of the range.}
		\label{figure:2d-almost-insensitive}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.7\textwidth]{images/speech-convergence.png}
		\caption{Convergence plot for the speech recognition experiment. The objective is shown in negative numbers because it is set to the negative normalized edit distance between the target and prediction, that is we are maximizing a negative loss.}
		\label{figure:speech-convergence}
	\end{center}
\end{figure}

In the last experiment we trained a speech recognition model on a single dataset. The hyperparameters in this case are however not only the parameters of the model itself, but of a pre-processing step that takes place right before the network is trained, with the capacity of the model kept fixed. We only optimize the ones that have a high chance of being influenced by the preprocessing without changing the model capacity, such as \emph{batch\_size} and \emph{dropout}.

\autoref{figure:2d-almost-insensitive} shows an example where we coincidentally chose the hyperparameter ranges such that only the values at the very edge have some influence over the objective function. If we were to set them only a little differently the regression could end up showing no relationship between the two hyperparameters. Setting the bounds on hyperparameters is an important but non-trivial effort. If we set the bounds too large, we end up wasting lots of computation time, as the volume of an $n$-dimensional hypercube grows exponentially. On the other hand, if we set the bounds too tight, the optimizer might only find flat regions of the objective function.

Similarly to the previous experiments the speech recognition model also has a steady convergence curve as shown in \autoref{figure:speech-convergence}. The best achieved value of the objective function was $-26.81$, which surpassed the hand-tuned baseline model achieving $-30.2$.


\chapter{Conclusion}

The goal of this thesis was to implement a practical tool for optimizing hyperparameters of neural networks using Bayesian optimization. We showed the theoretical foundations of Bayesian optimization, including the necessary mathematical background for Gaussian Process regression, and some of the extensions to Bayesian optimization such as parallel evaluations, as well as an architectural overview of our implementation. We also performed multiple experiments to evaluate the performance on different neural network architectures and tasks.

In our comparison to a random search (see \autoref{section:experiments-empirical-bayes}), Bayesian optimization usually obtained a higher objective function value, and achieved lower variance in repeated experiments. Furthermore, in three out of four experiments, the hyperparameters discovered by Bayesian optimization outperformed the manually designed ones. In the largest experiment, where we tried to train a network on multiple different datasets at once (see \autoref{section:multiple-treebanks-failed}), Bayesian optimization did not improve over the baseline. We hypothesize that Bayesian optimization was influenced by the introduced noise, and that the ability of directly modelling the noise does not mean, that the model will be immune to the noise and the noise will not affect its performance.

Furthermore, the underlying Gaussian Process regression offers a useful tool for visualizing the effects of each hyperparameter on the fitness function.


%While some of our results were positive, it remains unclear whether Bayesian optimization is a superior alternative to random search. Regardless, our tool remains flexible enough that should any future evidence supporting options such as better prior choices or acquisition functions arise, it could be easily incorporated within the optimization framework.


\section{Future Work}

Bayesian optimization is a broad area of research with many possibilities to be tried. In this thesis, we concentrated Gaussian processes as the probabilistic model of the objective function, mainly for limit the scope of the work. Other probabilistic models, like random forests, could be beneficial in the context of neural network hyperparameter optimization, having the ability to naturally capture categorical parameters, and perhaps blurring the lines between hyperparameter tuning and architecture search.

In the context of a GP regression, we also restricted ourselves to simple $\operatorname{Gamma}$ priors and point estimates for kernel parameter inference. Perhaps using Hamiltonian Monte Carlo for full Bayesian posterior inference, as implemented in \cite{gpy2014}, could alleviate some of our problems with unstable kernel parameters.

A GP regression could be further improved to properly handle integer and categorical variables (see \autoref{section:integer-hyperparameters}), which even though would be useful in practice, would most likely not affect the experiments we performed, given that the scale of our integer hyperparameters (mainly \emph{batch\_size}) was large.

