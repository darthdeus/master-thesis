Consider the problem of optimizing an arbitrary continuous function $f: ùìß ‚Üí ‚Ñù$
where $ùìß ‚äÇ ‚Ñù^d, d ‚àà ‚Ñï$. We call $f$ the \emph{objective function} and treat it
as a black box, making no assumption on its analytical form, or on our ability
to compute its derivatives. Our goal is to find the global minimum
$\symbf{x}_\text{opt}$ over the set $ùìß$, that is
$$
\symbf{x}_\text{opt} = \arg\min_{\symbf{x} ‚àà ùìß} f(\symbf{x}).
$$

We also assume that the evaluation of $f$ is expensive, as the goal of Bayesian
optimization is to find the optimum as quickly as possible.  Consider the case
when evaluating $f$ means performing a computation that is not only time
consuming, but for example also costs a lot of money. We might only have a
fixed budget which puts a hard limit on the number of evaluations we can
perform.

If the function can be evaluated cheaply, other global optimization approaches
such as simulated annealing or evolution strategies could potentially yield
better results (TODO ref).


Bayesian optimization techniques are some of the most efficient approaches in
terms of the number of function evaluations required. Much of the efficiency
stems from the ability to incorporate prior belief about the problem and to
trade of exploration and exploitation of the search space. [Nando 2012] It is
called Bayesian because it combines the prior knowledge $p(f)$ about the
function together with the data in the form of the likelihood $p(x|f)$ to
formulate a posterior distribution on the set of possible functions $p(f|x)$.
We will use the posterior distribution to figure out which point should be
evaluated next to give a likely improvement over the currently obtained
maximum.

\section{Acquisition Functions}

