\section{Acquisition functions}
\label{section:acq-fn}

An intuitive choice for an acquisition function is to maximize the probability
of improving over our currently best achieved value, which is called the
\newterm{probability of improvement}. This can be computed in closed form as
$$
PI(\vx) = Œ¶ \left( \frac{Œº(\vx) - y_{\max}}{œÉ(\vx)} \right)
$$
where $y_{\max}$ is the maximum value achieved by sampling $f(\vx)$.

A natural extension is the \newterm{expected improvement} acquisition function
which is simply the expected improvement over the currently achieved maximum.
We define
it as
$$
EI(x) = ùîº[f(x) - y_{\max}].
$$
At first it might seem that the expectation would be an intractable integral,
but fortunately even this equation can be computed in closed form as
$$
EI(x) = Œî(x) + œÉ(x) œÜ \left( \frac{Œî(x)}{œÉ(x)} \right) - |Œî(x)| Œ¶ \left( \frac{Œî(x)}{œÉ(x)} \right)
$$
where $Œî(x) = Œº(x) - y_{\max}$. In practice, improvement shows better results
than probability of improvement. For more examples of acquisition functions see
\cite{frazier2018tutorial}.


\section{Parallel evaluations}

\section{bopt algorithm}

matematika

\section{Integer parameters}

\section{Logscale}

% \subsection{Expected Improvement}
%
% \newterm{Expected improvement} $EI(x)$ is defined as
%
% \begin{equation}
%   EI(x) = E_{Y \sim \gN(\mu, \sigma^2)} [max(f(x) - f(x^+), 0)]
% \end{equation}
%
% where $f(x^+)$ is the value of the best sample so far and $x^+ =
% \argmax_{x_i \in x_{1:t}} f(x_i)$.
%
% \subsection{Probability of Improvement}
%
% We add a small term $\xi$ because otherwise $PI(x^+) = \frac{1}{2}.$
%
% \begin{align}
%   PI(x) &= P(f(x) \geq \mu^+ + \xi) \\
%         &= \Phi \left(\frac{\mu(x) - \mu^+ - \xi}{\sigma(x)} \right)
% \end{align}
%
% where $\Phi$ is the CDF of a Gaussian. If we knew up front what is the best
% value $f(x^*)$ we can model that directly by $P(f(x) \geq f(x^*))$.
%
%
% \subsection{GP-UCB}
%
% Define the \newterm{regret} and \newterm{cumulative regret} as follows:
%
% \begin{align}
%   r(x) &= f(x^*) - f(x) \\
%   R_T &= r(x_1) + \cdots + r(x_T).
% \end{align}
%
% The GP-UCB criterion is as follows:
%
% \begin{align}
%   GP-UCB(x) = \mu(x) + \sqrt{\nu \beta} \sigma(x)
% \end{align}
%
% with $\nu = 1$ and $\beta_t = 2 \log(t^{d/2 + 2}\pi^2 / 3 \delta)$ where it can
% be shown with high probability that this method has no regret, i.e. $\lim_{T
% \rightarrow \infty} R_T / T = 0$. (Srinivas et at, 2010 TODO REF). (NOTE, if
% the set of points is not infinite, $\beta$ ends up being different).
%
%
% \section{Algorithm}
%
% TODO
%
% \section{Parallel Evaluations}
%
% TODO
