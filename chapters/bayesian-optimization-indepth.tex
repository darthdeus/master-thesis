\chapter{Technical Details of BO}
\label{chapter:bo-indepth}

This chapter provides a more technical insight into Bayesian optimization. We begin by looking at the acquisition functions from a mathematical perspective, as they form the basis of Bayesian optimization. Next, we show how to extend Bayesian optimization to perform parallel evaluations, work with integer and discrete hyperparameters, and optimize parameters on a logarithmic scale. And finally, we explore the Bayesian optimization algorithm in detail, including some of its numerical properties and issues that can arise when implementing it.

The contents of this chapter are largely implementation details and are not necessary for one to use Bayesian optimization in practice. However, understanding the behavior of integer based hyperparameters, in addition to the overview in \autoref{section:architecture-search}, can prove useful when deciding if a certain hyperparameter makes for a good candidate for automatic tuning using Bayesian optimization.

\section{Acquisition functions}
\label{section:acq-fn}

The acquisition function is a key component of Bayesian optimization. Together with the GP regression it allows us to balance the exploration-exploitation problem in search. The only limitation we need to impose on the acquisition function is tractability, and possibly continuity, as we will need to optimize it. The tractability requirement is mandatory, as without being able to compute the function we could hardly find its maximum. But the continuity requirement is useful as the acquisition function is often optimized using stochastic gradient optimizers such as L-BFGS.

An intuitive choice for an acquisition function is to maximize the probability
of improving over our currently best achieved value, which is called the
\newterm{probability of improvement}. This can be computed in closed form as
$$
PI(\vx) = Œ¶ \left( \frac{Œº(\vx) - y_{\max}}{œÉ(\vx)} \right)
$$
where $y_{\max}$ is the maximum value achieved by sampling $f(\vx)$.

A natural extension is the \newterm{expected improvement} (EI) acquisition function
which is simply the expected improvement over the currently achieved maximum.
We define it as $$EI(x) = ùîº[f(x) - y_{\max}].$$ At first it might seem that
the expectation would be an intractable integral, but fortunately even this
equation can be computed in closed form as
$$
EI(x) = Œî(x) + œÉ(x) œÜ \left( \frac{Œî(x)}{œÉ(x)} \right) - |Œî(x)| Œ¶ \left( \frac{Œî(x)}{œÉ(x)} \right)
$$
where $Œî(x) = Œº(x) - y_{\max}$. In practice, improvement shows better results
than probability of improvement. For more examples of acquisition functions see
\cite{frazier2018tutorial}.

In both of these cases, the next sampling point would be chosen by maximizing
the acquisition function, that is $$x_{\operatorname{next}} = \argmax_x EI(x)$$ for
the case of expected improvement. This can again be done by any stochastic optimizer,
such as the commonly chosen L-BFGS with restarts.


\section{Parallel Evaluations}
\label{section:parallel-evaluations}

In practice we might have the ability to evaluate $f(\vx)$ at multiple points
in parallel, but the framework we have shown so far only allows for sequential
optimization. In the previous section we've shown a few examples of the
acquisition functions. A natural extension would be to not optimize with respect
to a single $x_{\operatorname{next}}$, but rather multiple points. In the context of
EI this is called \newterm{parallel expected improvemenet}.

Unfortunately, there is no simple solution \citep{frazier2018tutorial}. A
common solution is the so called \newterm{Constant Liar} approximation, which
chooses $x_{i+1}$ assuming $x_i$ was already chosen, and has the corresponding
value $y_i$ equal to a constant, often chosen to be the expected value of
$f(x_i)$ under the GP posterior.

This allows us to trivially implement parallel evaluations by simply
considering the $Œº$ prediction for unfinished evaluations as their $y$ value
and consider them part of the dataset $ùìì$.


\section{Integer Hyperparameters}

GP regression by itself does not have the ability to model integer values in $X$ directly as some other models do (e.g. random forests \autoref{chapter:bo}). A common solution, used by \citep{spearmint} and which we also implement in this thesis, is to consider all parameters to be real valued and only round them at the end.

In recently published work by \cite{integer-valued-gp} they show a more principled approach. The effect of rounding causes the model to see variation and relationships even among constant-valued regions. A possible downside is that the model could predict values different enough so that the acquisition function would obtain a maximum within a constant region which already has an existing sample, and thus wasting an evaluation. A proposed solution to this problem, as mentioned in the paper, is to round the appropriate values right before they are input into the kernel function, such as
$$\kappa'(x_1, x_2) = \kappa(T(x_1), T(x_2))$$ where $T(x)$ is an identity for real valued elements and a rounding function for integers.

Our implementation however does not use this approach, as our GP regression is handled by the \cite{gpy2014} library, which did not support it at the time, and implementing it would mean overriding many of the existing kernels. We did instead handle the problem explicitly by detecting the pathological cases, as described in \autoref{chapter:software}.


\section{Logarithmic Scaling}

When optimizing hyperparameters we might want to distinguish not only between real and integer valued ones, but also based on their scale. Optimizing the number of training epochs or layers are very well modelled by a linear scale, but a learning rate is better modelled with a logarithmic scale.

We provide a simple solution, which can work independently of Bayesian optimization, by simply transforming all of the appropriate value to logscale before inputting them into the model, and then transforming them back after we get a next sample $x$ proposal. As this approach is completely transparent from the point of the GP regression model, we could just as well perform any other arbitrary bijection.


\section{Implementation Details of Bayesian Optimization}
\label{section:bopt-alg}

We will now show the algorithm for Bayesian optimization in greater detail as compared to \autoref{alg:bopt}. Let $ùìì_n = \{ (\symbf{x}_i, y_i), i \in 1:n\}$ denote a set of $n$ samples (evaluations) of the objective function $f$, that is $y_i = f(\symbf{x}_i)$. Our goal is to pick the next $\symbf{x}_{n+1}$ to maximize our chance of finding the optimum quickly, assuming that already enough points were evaluated for us to fit the GP. \autoref{alg:bopt-detailed} shows one iteration of Bayesian optimization as it picks $\vx_{n+1}$.

\begin{algorithm}
	\label{alg:bopt-detailed}
	\DontPrintSemicolon
	\SetAlgoLined
	Let $\mX$ be the matrix of all $\vx_i$, and $\vy$ be a column vector of all $y_i$ \;
	Maximize the kernel log-likelihood $p(\mX) = -\frac{1}{2}\vy \mK^{-1}\vy - \frac{1}{2} \log \det \mK - \frac{N}{2} \log (2 œÄ)$, where $\mK = \kappa(\mX, \mX)$, by tuning the kernel hyperparameters \;
	Maximize the acquisition function $A(\mX, \vy, \mathcal{G}\mathcal{P})$ as a function of the GP using the kernel hyperparameters obtained in the previous step. \;
	Sample $y_{n+1} = f(\vx)$ where $\vx = \argmax A(\mX, \vy, \mathcal{G}\mathcal{P})$. \;
	Add $y_{n+1}$ to the dataset as $ùìì_{i+1} = ùìì_i \cup (\vx_{i+1}, y_{i+1})$.
	\caption{Bayesian Optimization with implementation details.}
\end{algorithm}

While the above shown algorithm is enough to explain how Bayesian optimization works, there are a few cases where numerical issues can arise, and we point them out next.

Firstly, the quadratic form $\vy \mK^{-1} \vy$ does not need to be computed using a matrix inverse procedure, which can be numerically unstable and requires more intermediate computation (see \cite{cholesky-inverse-krishnamoorthy2013matrix} for details). In general, consider the equation $\mA \vx = \vb$. We can instead write $\vx = \mA^{-1} \vb$ for an invertible matrix $\mA$. A naive solution would use a procedure for an inverse and then multiply to compute $\inv(\mA) \vb$, but because we do not actually need the inverse itself, but rather the multiplication, we can solve for $\mA^{-1} \vb$ directly using a \inlinecode{solve} procedure \citep{numpy}.

Because the $\mK$ matrix is a covariance matrix we also know it is positive definite. This allows us to take the computation one step further and compute its Cholesky factorization $\mL = \chol(\mK)$, and then use a procedure for \inlinecode{solve} directly on the factorized matrix $\mL$ (in TensorFlow available as \inlinecode{tf.linalg.cholesky\_solve}).

Having just computed the Cholesky factorization we can re-use it in the second expression of the marginal log-likelihood, that is the $\log \det \mK$, since
\begin{align}
\log \det \mK &= \log \det(\mL^T \mL) = \log(\det(\mL^T) \cdot \det(\mL)) = \\
&= 2 \cdot \log \det \mL = 2 \cdot \log \prod \diag \mL = 2 \cdot \sum \log \diag. \mL
\end{align}
This allows us to compute the determinant, which is usually $O(n^3)$, in just linear time, because we're re-using the work already being done in the Cholesky factorization used in the previous step.

Another interesting note, discovered by our initial custom implementation, and confirmed in the implementation of the GPy \citep{gpy2014} library, is that when computing a Cholesky factorization of a covariance matrix computed on real world data, it will often fail for numerical reasons, and requires additional noise to be added to the diagonal to improve stability. What GPy does internally is to iteratively increase the amount of noise, up until some threshold, to make sure the factorization succeeds without any problems, while not adding excessive noise when not needed.

Lastly, the optimization procedures themselves used for maximizing the kernel log-likelihood and acquisition function can be sensitive. Our initial implementation in SciPy and TensorFlow showed very different restarts based on the type of optimizer (SGD, Adam, L-BFGS), and its meta-parameters, such as the number of restarts, stop tolerance criterion, learning rates, etc. These problems, along with the many numerical issues encountered along the way, contributed to the choice of using GPy for the final implementation. We outline a few more reasons in \autoref{section:gpy}.