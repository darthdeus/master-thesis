\section{Acquisition functions}
\label{section:acq-fn}

An intuitive choice for an acquisition function is to maximize the probability
of improving over our currently best achieved value, which is called the
\newterm{probability of improvement}. This can be computed in closed form as
$$
PI(\vx) = Œ¶ \left( \frac{Œº(\vx) - y_{\max}}{œÉ(\vx)} \right)
$$
where $y_{\max}$ is the maximum value achieved by sampling $f(\vx)$.

A natural extension is the \newterm{expected improvement} (EI) acquisition function
which is simply the expected improvement over the currently achieved maximum.
We define it as $$EI(x) = ùîº[f(x) - y_{\max}].$$ At first it might seem that
the expectation would be an intractable integral, but fortunately even this
equation can be computed in closed form as
$$
EI(x) = Œî(x) + œÉ(x) œÜ \left( \frac{Œî(x)}{œÉ(x)} \right) - |Œî(x)| Œ¶ \left( \frac{Œî(x)}{œÉ(x)} \right)
$$
where $Œî(x) = Œº(x) - y_{\max}$. In practice, improvement shows better results
than probability of improvement. For more examples of acquisition functions see
\cite{frazier2018tutorial}.

In both of these cases, the next sampling point would be chosen by maximizing
the acquisition function, that is $$x_{\operatorname{next}} = \argmax_x EI(x)$$ for
the case of expected improvement. This can again be done by any stochastic optimizer,
such as the commonly chosen L-BFGS with restarts.


\section{Parallel evaluations}
\label{section:parallel-evaluations}

In practice we might have the ability to evaluate $f(\vx)$ at multiple points
in parallel, but the framework we have shown so far only allows for sequential
optimization. In the previous section we've shown a few examples of the
acquisition functions. A natural extension would be to not optimize with respect
to a single $x_{\operatorname{next}}$, but rather multiple points. In the context of
EI this is called \newterm{parallel expected improvemenet}.

Unfortunately, there is no simple solution \citep{frazier2018tutorial}. A
common solution is the so called \newterm{Constant Liar} approximation, which
chooses $x_{i+1}$ assuming $x_i$ was already chosen, and has the corresponding
value $y_i$ equal to a constant, often chosen to be the expected value of
$f(x_i)$ under the GP posterior.

This allows us to trivially implement parallel evaluations by simply
considering the $Œº$ prediction for unfinished evaluations as their $y$ value
and consider them part of the dataset $ùìì$.


\section{Integer parameters}

GP regression by itself does not have the ability to model integer values in $X$ directly as some other models do (e.g. random forests \autoref{chapter:bo}). A common solution, used by \citep{spearmint} and which we also implement in this thesis, is to consider all parameters to be real valued and only round them at the end.

In recently published work by \cite{integer-valued-gp} they show a more principled approach. The effect of rounding causes the model to see variation and relationships even among constant-valued regions. A possible downside is that the model could predict values different enough so that the acquisition function would obtain a maximum within a constant region which already has an existing sample, and thus wasting an evaluation. A proposed solution to this problem, as mentioned in the paper, is to round the appropriate values right before they are input into the kernel function, such as
$$\kappa'(x_1, x_2) = \kappa(T(x_1), T(x_2))$$ where $T(x)$ is an identity for real valued elements and a rounding function for integers.

Our implementation however does not use this approach, as our GP regression is handled by the \cite{gpy2014} library, which did not support it at the time, and implementing it would mean overriding many of the existing kernels. We did instead handle the problem explicitly by detecting the pathological cases, as described in \autoref{chapter:software}.


\section{bopt algorithm}
\label{section:bopt-alg}

matematika

\section{Logscale}

When optimizing hyperparameters we might want to distinguish not only between real and integer valued ones, but also based on their scale. Optimizing the number of training epochs or layers are very well modelled by a linear scale, but a learning rate is better modelled with a logarithmic scale.

We provide a simple solution, which can work independently of Bayesian optimization, by simply transforming all of the appropriate value to logscale before inputting them into the model, and then transforming them back after we get a next sample $x$ proposal.

