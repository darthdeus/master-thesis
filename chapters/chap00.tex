\chapter{Introduction}
\label{chapter:bo}

intro (2 stranky)
  - co jsou hyperparam, ze to model neumi nastavit, atd.
  - proc chceme delat black box, ...
\\

\input{chapters/introduction.tex}


\chapter{Bayesian Optimization}

\input{chapters/bayesian-optimization-overview.tex}


\chapter{Gaussian Processes}
\label{chapter:gp}

- uvod, proc to delame
- ucbnicove
- kernely existujou
\\

\input{chapters/gaussian-processes.tex}


\chapter{Bayesian Optimization in depth}

- bopt alg
- jaky kernely v bayes opt., co pouzivame, proc (ref)
- paralelni evaluace
\\

\input{chapters/bayesian-optimization-indepth.tex}


\chapter{Software}

- co umime
- vizualizace
- jak se to pousti, runnery, serializace
\\

\input{chapters/software.tex}

\chapter{Experiments}

- toy tasky
  - porovnani existujici fuj fce na optimalizaci
  - srovnani acq/kernel, random search (ze to neco dela)

- maly ulohy

- velka uloha

- interpretace vysledku
\\

\input{chapters/experiments.tex}



- parser
- tokenizer/segmentace
- speech recognition
- opennmt lemmatizace
- $reinforce_with_baseline$





---

Let $ùìì_n = \{ (\symbf{x}_i, y_i), i \in 1:n\}$ denote a set of $n$ samples
(evaluations) of the function $f$, that is $y_i = f(\symbf{x}_i)$. Our goal is
to pick the next $\symbf{x}_{n+1}$ to maximize our chance of finding the
optimum quickly.

Consider the set of all continuous functions $f ‚àà ùìï$ with a prior distribution
$p(f)$.  Conditioning on our samples gives us a posterior distribution over
possible functions $p(f | ùìì)$.

---

