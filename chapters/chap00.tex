\section{Bayesian Optimization}

Consider the problem of optimizing an arbitrary continuous function $f: ğ“§ â†’ â„$
where $ğ“§ âŠ‚ â„^d, d âˆˆ â„•$. We call $f$ the \emph{objective function} and treat it
as a black box, making no assumption on its analytical form, or on our ability
to compute its derivatives. Our goal is to find the global minimum
$\symbf{x}_\text{opt}$ over the set $ğ“§$, that is

$$
  \symbf{x}_\text{opt} = \arg\min_{\symbf{x} âˆˆ ğ“§} f(\symbf{x}).
$$

We also assume that the evaluation of $f$ is expensive, as the goal of Bayesian
optimization is to find the optimum as quickly as possible. If the function can
be evaluated cheaply, other global optimization approaches such as simulated
annealing or evolution strategies could potentially yield better results (TODO
ref).

Consider the case when evaluating $f$ means performing a computation that is
not only time consuming, but for example also costs a lot of money. We might
only have a fixed budget which puts a hard limit on the number of evaluations
we can perform.

Bayesian optimization techniques are some of the most efficient approaches in
terms of the number of function evaluations required. Much of the efficiency
stems from the ability to incorporate prior belief about the problem and to
trade of exploration and exploitation of the search space. [Nando 2012] It is
called Bayesian because it combines the prior knowledge $p(f)$ about the
function together with the data in the form of the likelihood $p(x|f)$ to
formulate a posterior distribution on the set of possible functions $p(f|x)$.
We will use the posterior distribution to figure out which point should be
evaluated next to give a likely improvement over the currently obtained
maximum.

---

Let $ğ““_n = \{ (\symbf{x}_i, y_i), i \in 1:n\}$ denote a set of $n$ samples
(evaluations) of the function $f$, that is $y_i = f(\symbf{x}_i)$. Our goal is
to pick the next $\symbf{x}_{n+1}$ to maximize our chance of finding the
optimum quickly.

Consider the set of all continuous functions $f âˆˆ ğ“•$ with a prior distribution
$p(f)$.  Conditioning on our samples gives us a posterior distribution over
possible functions $p(f | ğ““)$.

---

