\section{Bayesian Optimization}

Consider the problem of optimizing an arbitrary Lipschitz continuous function
$f: 𝓧 → ℝ$ where $𝓧 ⊂ ℝ^d, d ∈ ℕ$. We call $f$ the \emph{objective function}
and treat it as a black box. That is, we make no assumption on its analytical
form, or on our ability to compute its derivatives. The main assumption is on
its continuity in order to approximate it with a regression model. Our goal is
to find the global minimum $\symbf{x}_\text{opt}$ over the set $𝓧$, that is

$$
  \symbf{x}_\text{opt} = \arg\min_{\symbf{x} ∈ 𝓧} f(\symbf{x}).
$$

We also assume that the evaluation of $f$ is expensive, as the goal of Bayesian
Optimization is to find the optimum as quickly as possible. If the function can
be evaluated cheaply, other global optimization approaches such as simulated
annealing or evolution strategies could potentially yield better results (TODO
ref).

The optimum is found by sequentially evaluating $f$ at at new points

Let $𝓓_n = \{ (\symbf{x}_i, y_i), i \in 1:n\}$ denote a set of $n$ samples
(evaluations) of the function $f$, that is $y_i = f(\symbf{x}_i)$. Our goal is
to pick the next $\symbf{x}_{n+1}$ to maximize our chance of finding the
optimum quickly.

Consider the set of all continuous functions $f ∈ 𝓕$ with a prior distribution $p(f)$.
Conditioning on our samples gives us a posterior distribution over possible
functions $p(f | 𝓓)$.
