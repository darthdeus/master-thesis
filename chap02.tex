\chapter{Gaussian distribution}

\begin{defn}
\newterm{Univariate Gaussian distribution} is:

\begin{equation}
    \rX \sim \gN(\mu, \sigma^2), \mu \in \sR, \sigma^2 > 0
\end{equation}

means, $\rX$ has density

\begin{equation}
    f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{ \left\{ -\frac{1}{2\sigma^2} (x - \mu)^2 \right\}}
\end{equation}
\end{defn}

\begin{defn}
\newterm{Degenerate Univariate Gaussian} is:

\begin{equation}
    \rX \sim \gN(\mu, 0)
\end{equation}

if $\rX \equiv \mu$ (i.e.\ $X(\omega) = \mu, \forall \omega \in \Omega$)
\end{defn}

\begin{defn}
A random variable $\rX \in \sR^n$ is a \newterm{multivariate Gaussian} if any linear combination of its components is univariate Gaussian, i.e. $\va^T \mX = \sum_{i=1}^n \va_i \mX_i$ is Gaussian ($\forall \va \in \mR^n$).

\emph{This is apparently a more general definition than the usual definition with a density, because it allows for a degenerate multivariate Gaussian}
\end{defn}

\begin{defn}
$\rX \sim \gN(\vmu, \mSigma)$ means $X$ is a Gaussian with $E[\rX_i] = \vmu_i$, and $cov(\rX_i, \rX_j) = \mSigma_{ij}$. Note that this implies that $\mSigma$ is positive semi-definite.
\end{defn}

Note that $\vmu$ and $\mSigma$ uniquely determine the distribution $\gN(\vmu, \mSigma)$. \todo{proof}

\begin{defn}
$\rX \sim \gN(\vmu, \mSigma)$ is \newterm{degenerate multivariate Gaussian}, if $\det \mSigma = \mZero$.
\end{defn}

\section{Independent components of a Gaussian}

$\rX_1, \dots, \rX_n$ are independent with $\rX_i \sim \gN(\mu_i, \sigma_i^2)$ iff $\rX = (\rX_1, \dots, \rX_n) \sim \gN(\vmu, \mSigma)$, where $\vmu = (\mu_1, \dots, \mu_n)$ and $\mSigma = diag(\sigma_1^2, \dots, \sigma_n^2)$

\begin{thm} If $\rX \in \mR^n$ is Gaussian, then $\rX_i, \rX_j$ are independent iff $cov(\rX_i, \rX_j) = 0$. Note this is not true in general, and is a special property of the Gaussian.

\todo{proof}
\end{thm}

\begin{thm}
$\rX_1, \dots, \rX_n$ each univariate Gaussian \emph{does not imply} $\rX = (\rX_1, \dots, \rX_n)$ is a multivariate Gaussian.

\todo{wording, proof}
\end{thm}

\begin{thm}
A Gaussian random variable $\rX \sim \gN(\vmu, \mSigma)$ has a density iff it is non-degenerate (i.e.\ $\det \mSigma \neq 0$, alternatively $\mSigma$ is positive-definite).

And in this case,

\begin{equation}
    f(\vx) = \frac{1}{\sqrt{\det(2 \pi \mSigma)}} \exp{ \left\{ - \frac{1}{2} (\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu) \right\} }
\end{equation}

Note that $\det(2 \pi \mSigma) = (2\pi)^n \det(\mSigma)$. Alternatively $(2 \pi)^{n/2} (\det \mSigma)^{1/2}$ \todo{check and proof}. Also note that $\det \mSigma \neq$ implies that $\mSigma$ is invertible, which conincides with the density requiring an invertible $\mSigma$.

Also note that if $n = 1$, then $\mSigma = \sigma^2$, $cov(X, X) = \sigma^2 x$, $\mSigma^{-1} = \frac{1}{\sigma^2}$, and hence the multivariate Gaussian formula becomes the univariate one.

\begin{equation}
    \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{\left\{ - \frac{1}{2} \frac{1}{\sigma^2} (x - \mu)^2\right\}}
\end{equation}
\end{thm}

The term $(\vx - \vmu)^T \mSigma^{-1}(\vx - \vmu)$ is called a Mahalanobis distance, and is also a quadratic form in $x$. A general quadratic form is $\vx^T \mA \vx$. \todo{rewrite, derive that gaussian is a hyper-ellipsoid}.

Prove, that when $\mA$ is positive semi-definite, the shape is an ellipsoid. \todo{small appendix on quadratic forms and Mahalanobis distance}.

We can think of a gaussian as $\exp{\left\{ \text{quadratic form} \right\} }$. \todo{prove that covariance matrix is positive semi-definite}. \todo{prove, that if individual components have positive variances, the joint will be diagonal positive definite}.

\begin{defn}
\newterm{Affine transformation}:

\begin{equation}
    f(\vx) = \mA \vx + \vb
\end{equation}
\end{defn}

\begin{thm}
\newterm{Affine property}: Any affine transformation of a Gaussian is a Gaussian. That is, if $\rX \sim \gN(\vmu, \mSigma)$, then $\mA \rX + \vb \sim \gN(\mA \vmu + \vb, \mA \mSigma \mA^T)$ (for any $\vmu \in \mR^n, \mSigma \in \mR^{n \times n}$ positive semi-definite, and any $\mA \in \mR^{m \times n}, \vb \in \mR^m$.

(Constructing) $\rX_1, \dots, \rX_n \sim \gN(0, 1)$ independent $\implies \rX \sim \gN(0, \mI)$, which also implies $\mA \rX + \vmu \sim \gN(\vmu, \mSigma)$ where $\mSigma = \mA \mA^T$ (for any $\vmu \in \mR^n, \mA \in \mR^{m \times n}$).
\end{thm}

\begin{thm}
\newterm{Sphering}: If $\mSigma$ is positive-definite, then:

\begin{equation}
    \mY \sim \gN(\vmu, \mSigma) \implies \mA^{-1} (\mY - \vmu) \sim \gN(0, \mI) \text{where} \mSigma = \mA \mA^T
\end{equation}

The random variable $\mA^-1 (\mY - \vmu)$ is then a unit sphere in $n$-dimensional space.
\end{thm}

\section{Geometric intuition}

Let $\mX \sim \gN(0, \mI)$. Let $\mSigma$ be a covariance matrix, and $\vmu \in \mR^n$. Covariance matrix is always positive semi-definite, which means its eigenvalues are non-negative \todo{better text}. Because $\mSigma$ is a real, symmetric, positive semi-definite matrix it can be written as $\mSigma = \mQ \mLambda \mQ^T$ where $\mQ$ is an orthogonal matrix of eigenvectors, and $\mLambda$ is a diagonal matrix of eigenvalues \todo{proof in appendix}.

We can then do a simple algebraic manipulation to get the square root of $\mSigma = \mA \mA^T$. \todo{equivalent to cholesky decomp?}

\begin{equation}
    \mSigma = \mQ \mLambda \mQ^T = \mQ \mLambda^{1/2} \mLambda^{1/2} \mQ = (\mQ \mLambda^{1/2}) (\mQ \mLambda^{1/2})^T = \mA \mA^T
\end{equation}

Now we can do $\mY = \mA \mX + \vmu \implies \mY \sim \gN(\vmu, \mSigma)$.

Draw level sets of $\mX \sim \gN(0, \mI)$, $\mLambda^{1/2} \mX \sim \gN(0, \mLambda)$, $\mQ \mLambda^{1/2} \mX \sim \gN(0, \mSigma)$. $\mQ$ contains the eigenvectors of $\mSigma$, and multiplication by $\mQ$ rotates (every orthogonal matrix is equivalent to a rotation \todo{proof appendix}) the variable so that the axes are aligned to the eigenvectors. In other words, $\mQ$ is a change of basis from the eigenvector basis to the canonical basis, causing the rotation. $\mQ \mLambda^{1/2} \mX + \vmu \sim \gN(\vmu, \mSigma)$, in which case $\vmu$ simply shifts the whole coordinate space by $\vmu$, which in the case of a Gaussian only affects the mean \todo{proof}.

In summary, the eigenvalues in $\mLambda$ scale, $\mQ$ rotates in the direction of the eigenvectors, and $\vmu$ shifts the origin.

\begin{equation}
    (\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu) = (\vx - \vmu)^T \mQ \mLambda^{-1} \mQ (\vx - \vmu)
\end{equation}

\begin{equation}
    \mLambda^{-1} = \begin{bmatrix}
        \lambda_1^{-1} & & \\
        & \ddots & \\
        & & \lambda_n^{^-1}
    \end{bmatrix}
\end{equation}

TODO: Note the order of eigenvalues: large eigenvalue of $\mSigma$ is the direction with the ???smallest??? variance?

\section{Marginal and Conditional distribution}

If $\mX = (\mX_1, \mX_2) \in \sR^2$ are jointly Gaussian, then $\mX_1, \mX_2$ individually are also Gaussian. \todo{write it as a theorem}

\begin{proof}
By the affine property, $\mA \mX + \vb$ is always a Gaussian. Let us set $\mA = (\mI 0), \vb = 0$. We get $\gN(\mA \vmu + \vb, \mA \mSigma \mA^T)$ and see that both $\mX_1$ and $\mX_2$ are Gaussian.

More generally $\mX \sim \gN(\vmu, \mSigma), \va = (1, \dots, k), b = (k+1, \dots, n), 1 \leq k \leq n$.

\begin{align}
    \mX &= \begin{bmatrix} \mX_a \\ \mX_b \end{bmatrix},
    \mX_a = \begin{bmatrix} \mX_1 \\ \vdots \\ \mX_k \end{bmatrix},
    \mX_b = \begin{bmatrix} \mX_{k+1} \\ \vdots \\ \mX_n \end{bmatrix} \\
    \vmu &= \begin{bmatrix} \vmu_a \\ \vmu_b \end{bmatrix} \\
    \mSigma &= \begin{bmatrix}
        \mSigma_{aa} & \mSigma_{ab} \\
        \mSigma_{ba} & \mSigma_{bb}
    \end{bmatrix}
\end{align}

We then show that:

\begin{equation}
\mX_a \sim \gN(\vmu_a, \mSigma_{aa})
\end{equation}

This is true because can use the affine property with a projection matrix on the first $k$ dimensions.

\begin{equation}
    \mA = \begin{bmatrix} \mI_k 0 \end{bmatrix}, \quad \mA \in \sR^{k \times n}
\end{equation}

By construction, $\mA \mX = \mX_a \sim \gN(\mA \vmu, \mA \mSigma \mA^T)$, where $\mA \vmu = \mu_a$, and $\mA \mSigma \mA^T = \mSigma_{aa}$, and thus $\mX_a \sim \gN(\vmu_a, \mSigma_{aa})$.

\end{proof}