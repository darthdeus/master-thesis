\chapter{Gaussian distribution}


\section{Univariate Gaussian distribution}

\begin{defn}
Random variable $X$ has a \newterm{Univariate Gaussian distribution}, written as $\rX \sim \gN(\mu, \sigma^2), \mu \in \sR, \sigma^2 > 0$.

which means $X$ has a density

\begin{equation}
    f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{ \left\{ -\frac{1}{2\sigma^2} (x - \mu)^2 \right\}}.
\end{equation}
\end{defn}

\begin{defn}
\newterm{Degenerate Univariate Gaussian} is:

\begin{equation}
    \rX \sim \gN(\mu, 0)
\end{equation}

if $\rX \equiv \mu$ (i.e.\ $X(\omega) = \mu, \forall \omega \in \Omega$)
\end{defn}

\begin{defn}
A random variable $\rX \in \sR^n$ is a \newterm{multivariate Gaussian} if any linear combination of its components is univariate Gaussian, i.e. $\va^T \mX = \sum_{i=1}^n \va_i \mX_i$ is Gaussian ($\forall \va \in \mR^n$).

\url{https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Definition}

\todo{This is apparently a more general definition than the usual definition with a density, because it allows for a degenerate multivariate Gaussian}
\end{defn}


\begin{tcolorbox}
    \section{Multivariate Gaussian Distribution}
    
    In the previous section we derived the Gaussian distribution for a single variable $x$, which has the following density function: \todo{better explain the normalization constant}
    
    \begin{equation}
    \gN(x | \mu, \sigma^2) = \frac{1}{(2 \pi \sigma^2)^{1/2}} \exp \left( \frac{1}{2\sigma^2} (x - \mu)^2 \right) \label{eq:single-gaussian}
    \end{equation}
    
    where $\mu$ is the mean and $\sigma^2$ is the variance. In the case where $\vx$ is a $D$-dimensional vector, the multivariate Gaussian takes the following form:
    
    \begin{equation}
    \gN(x | \mu, \mSigma) = \frac{1}{(2 \pi)^{D/2} |\mSigma|^{1/2}} \exp \left( -\frac{1}{2} (\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu) \right) \label{eq:multi-gaussian}
    \end{equation}
    
    where $\vmu$ is a $D$-dimensional vector, $\mSigma$ is a $D \times D$ covariance matrix, and $|\mSigma|$ is the determinant of $\mSigma$.
\end{tcolorbox}


\begin{defn}
$\rX \sim \gN(\vmu, \mSigma)$ means $X$ is a Gaussian with $E[\rX_i] = \vmu_i$, and $cov(\rX_i, \rX_j) = \mSigma_{ij}$. Note that this implies that $\mSigma$ is positive semi-definite.
\end{defn}

Note that $\vmu$ and $\mSigma$ uniquely determine the distribution $\gN(\vmu, \mSigma)$. \todo{proof}

\begin{defn}
$\rX \sim \gN(\vmu, \mSigma)$ is \newterm{degenerate multivariate Gaussian}, if $\det \mSigma = \mZero$.
\end{defn}


\begin{tcolorbox}
    \section{Independent components of a Gaussian}
    
    $\rX_1, \dots, \rX_n$ are independent with $\rX_i \sim \gN(\mu_i, \sigma_i^2)$ iff $\rX = (\rX_1, \dots, \rX_n) \sim \gN(\vmu, \mSigma)$, where $\vmu = (\mu_1, \dots, \mu_n)$ and $\mSigma = diag(\sigma_1^2, \dots, \sigma_n^2)$
    
    \begin{thm} If $\rX \in \mR^n$ is Gaussian, then $\rX_i, \rX_j$ are independent iff $cov(\rX_i, \rX_j) = 0$. Note this is not true in general, and is a special property of the Gaussian.
    
    \todo{proof}
    \end{thm}
    
    \begin{thm}
    $\rX_1, \dots, \rX_n$ each univariate Gaussian \emph{does not imply} $\rX = (\rX_1, \dots, \rX_n)$ is a multivariate Gaussian.
    
    \todo{wording, proof}
    \end{thm}
    
    \begin{thm}
    A Gaussian random variable $\rX \sim \gN(\vmu, \mSigma)$ has a density iff it is non-degenerate (i.e.\ $\det \mSigma \neq 0$, alternatively $\mSigma$ is positive-definite).
    
    And in this case,
    
    \begin{equation}
        f(\vx) = \frac{1}{\sqrt{\det(2 \pi \mSigma)}} \exp{ \left\{ - \frac{1}{2} (\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu) \right\} }
    \end{equation}
    
    Note that $\det(2 \pi \mSigma) = (2\pi)^n \det(\mSigma)$. Alternatively $(2 \pi)^{n/2} (\det \mSigma)^{1/2}$ \todo{check and proof}. Also note that $\det \mSigma \neq$ implies that $\mSigma$ is invertible, which conincides with the density requiring an invertible $\mSigma$.
    
    Also note that if $n = 1$, then $\mSigma = \sigma^2$, $cov(X, X) = \sigma^2 x$, $\mSigma^{-1} = \frac{1}{\sigma^2}$, and hence the multivariate Gaussian formula becomes the univariate one.
    
    \begin{equation}
        \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{\left\{ - \frac{1}{2} \frac{1}{\sigma^2} (x - \mu)^2\right\}}
    \end{equation}
    \end{thm}
    
    The term $(\vx - \vmu)^T \mSigma^{-1}(\vx - \vmu)$ is called a Mahalanobis distance, and is also a quadratic form in $x$. A general quadratic form is $\vx^T \mA \vx$. \todo{rewrite, derive that gaussian is a hyper-ellipsoid}.
    
    Prove, that when $\mA$ is positive semi-definite, the shape is an ellipsoid. \todo{small appendix on quadratic forms and Mahalanobis distance}.
    
    We can think of a gaussian as $\exp{\left\{ \text{quadratic form} \right\} }$. \todo{prove that covariance matrix is positive semi-definite}. \todo{prove, that if individual components have positive variances, the joint will be diagonal positive definite}.
\end{tcolorbox}

We note here that the functional dependence of the Gaussian on $\vx$ is through the quadratic form

\begin{equation}
\Delta^2 = (\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu)
\end{equation}

which appears in the exponent \citep{bishop2016pattern} \todo{rewrite and remove the quote, or keep it?}. The quantity $\Delta$ is called the \newterm{Mahalanobis distance} from $\vmu$ to $\vx$ and reduces to the Euclidean distance when $\mSigma$ is the identity matrix. We will make use of the quadratic form in the following section when we derive the conditional and marginal distribution of the multivariate Gaussian.

Since $\mSigma$ is a covariance matrix we know it is positive definite \todo{why not only semidefinite? describe the degenerate case}, we can perform \newterm{eigendecomposition} on it to get $\mSigma = \mU \mLambda \mU^T$, where $\mU$ is an ortogonal matrix of eigenvectors, and $\mLambda$ is a diagonal matrix of eigenvalues. Basic matrix algebra gives us the following:

\begin{equation}
\mSigma^{-1} = (\mU^T)^{-1} \mLambda^{-1} \mU^{-1} = \mU \mLambda^{-1} \mU^T = \sum_{i = 1}^D \frac{1}{\lambda_i} \vu_i \vu_i^T
\end{equation}

where the second to last equality comes from $\mU$ being orthogonal ($\mU^{-1} = \mU^T$). We can use this to obtain a different form of the Mahalanobis distance: \todo{cite bishop?}

\begin{align}
(\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu) &= (\vx - \vmu)^T \left( \sum_{i = 1}^D \frac{1}{\lambda_i} \vu_i \vu_i^T \right) (\vx - \vmu) \\
&= \sum_{i = 1}^D (\vx - \vmu)^T \frac{1}{\lambda_i} \vu_i \vu_i^T (\vx - \vmu) \\
&= \sum_{i = 1}^D \frac{y_i^2}{\lambda_i} \label{eq:mvn-ellipse}
\end{align}

where $y_i = u_{i}^{T} (\vx - \vmu)$ \todo{use a notation for definition}. The equation \eqref{eq:mvn-ellipse} has exactly the same form as a $D$-dimensional ellipse. From this we conclude that the contour lines of a multivariate Gaussian will be elliptical, where the eigenvectors determine the orientation of the ellipse, and the eigenvalues determine its radius \todo{better word?} along each eigenvector.

\todo{complete the properties and derivation}

\section{Conditional and Marginal Gaussian Distribution}

In this section we derive the conditional $p(x_1 | x_2)$ and marginal $p(x_1)$ for a given joint distribution $p(x_1, x_2)$. One of the interesting properties of a multivariate Gaussian is that both the conditional and the marginal are also Gaussian and we can easily compute their parameters in closed from based on the parameters of the joint distribution.

Supposed $\vx$ is a $D$-dimensional random vector with a Gaussian distribution $\gN(\vx | \vmu, \mSigma)$, and that $\vx$ is partitioned into two vectors $\vx_1$ and $\vx_2$ such that

\begin{equation}
\vx = \partx
\end{equation}

We also partition the mean vector $\vmu$ and the covariance matrix $\mSigma$ into a block matrix. We also define the inverse of the covariance matrix $\mLambda = \mSigma^{-1}$, which will simplify a few of the equations that follow. We will derive the exact form of $\mLambda$ and of its individual blocks later in this section. For now we simply use the fact that $\mSigma$ is positive-definite, and thus it is invertible. The matrix $\mLambda$ is also known as a \newterm{precision matrix}.

\begin{equation}
\vmu = \partmu,
\mSigma = \partsigma, \mLambda = \mSigma^{-1} = \partlambda \label{eq:mvn-partition}
\end{equation}

Note that since $\mSigma$ is a symmetric matrix, $\ms{12}^T = \ms{21}$, and similarly $\ml{12}^T = \ml{21}$. Similarly, $\ms{11}$, $\ms{22}$, $\ml{11}$, and $\ml{22}$ are all symmetrical.

Before we derive the parameters of the conditional, we show that the conditional distribution $p(x_1 | x_2)$ is a Gaussian. To do this, we take the joint distribution $p(x_1, x_2)$ and fix the value of $x_2$ \citep{bishop2016pattern}. Using the definition of conditional probability $p(x_1, x_2) = p(x_1 | x_2) p(x_2)$ we can see that after fixing the value of $x_2$, $p(x_2)$ is simply a normalization constant, and the remaining term $p(x_1 | x_2)$ is a function of $x_1$ which together with the normalization constant gives us the conditional probability distribution on $x_1$.

We now use the partitioned form of the multivariate Gaussian defined by \eqref{eq:mvn-partition} to show that $p(x_1 | x_2)$ is actually a Gaussian. Let us begin by looking at the exponent in \eqref{eq:multi-gaussian}:

\begin{align}
-\frac{1}{2} (\vx - \vmu)^T \mLambda (\vx - \vmu) &= 
-\frac{1}{2} \left(\partx - \partmu \right)^T \partlambda \left(\partx - \partmu \right) \\
&= -\frac{1}{2} \partxmu^T \partlambda \partxmu
\end{align}

To make the next few equations easier to follow we set $\vy_1 = \vx_1 - \vmu_1$ and $\vy_2 = \vx_2 - \vmu_2$.

\begin{align}
\begin{split}
-\frac{1}{2} \begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix} ^T \partlambda \begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix} ={}& -\frac{1}{2} \begin{bmatrix} \vy_1 \ml{11} + \vy_2 \ml{21} \\ \vy_1 \ml{12} + \vy_2 \ml{22} \end{bmatrix} ^T \begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix}
\end{split} \\
%
\begin{split}
={}& -\frac{1}{2} \left( \vy_1^T \ml{11} \vy_1 + \vy_2^T \ml{21} \vy_1 + \vy_1^T \ml{12} \vy_2 + \vy_2^T \ml{22} \vy_2 \right)
\end{split} \\
%
\begin{split}
={}& -\frac{1}{2} (\vx_1 - \vmu_1)^T \ml{11} (\vx_1 - \vmu_1) {}+ \\
  &-\frac{1}{2} (\vx_2 - \vmu_2)^T \ml{21} (\vx_1 - \vmu_1) {}+ \\
  &-\frac{1}{2} (\vx_1 - \vmu_1)^T \ml{12} (\vx_2 - \vmu_2) {}+ \\
  &-\frac{1}{2} (\vx_2 - \vmu_2)^T \ml{22} (\vx_2 - \vmu_2) \label{eq:mvn-quadratic-form}
\end{split}
\end{align}

\todo{fix + alignment and equation numbering, fix spacing of + in 1.29}

We see that this is a quadratic form in $x_1$, and hence the corresponding conditional distribution $p(x_1 | x_2)$ will be Gaussian. \todo{better explain, cite bishop again?}. We can use \eqref{eq:mvn-quadratic-form} to derive the parameters of $p(x_1 | x_2)$ since

\todo{partition matrix inversion lemma}

\citep{murphy2012machine}

\section{Sum of Gaussians and Other Properties}

\todo{better section name}

\begin{defn}
The \newterm{moment generating function} of a random variable $X$ is

\begin{equation}
M_X(t) = \E[e^{t X}].
\end{equation}
\end{defn}

We state the following result without proof \todo{citace, pridat proof}.

\begin{thm}
Let $X$ and $Y$ be two random variables. If

\begin{equation}
M_X(t) = M_Y(t)
\end{equation}

for all $t \in (-\delta, \delta)$ for some $\delta > 0$, then $X$ and $Y$ have the same distribution. We'll use this theorem to show that the sum of Gaussian random variables has a Gaussian distribution.
\end{thm}

\begin{thm} \citep{mitzenmacher2017probability}\label{thm:sum-independent-gaussian}
If $X$ and $Y$ are independent random variables, then

\begin{equation}
M_{X + Y}(t) = M_X(t) M_Y(t)
\end{equation}
\end{thm}
\begin{proof}
\begin{equation}
M_{X + Y}(t) = \E[e^{t(X + Y)}] = \E[e^{tX} e^{tY}] = \E[e^{tX}] \E[e^{tY}] = M_X(t) M_Y(t)
\end{equation}

Here we have used that $X$ and $Y$ are independent -- and hence $e^{tX}$ and $e^{tY}$ are independent -- to conclude that $\E[e^{tX} e^{tY}] = \E[e^{tX}] \E[e^{tY}]$. \todo{prepsat? prakticky opsano}
\end{proof}

\begin{thm} Moment generating function of a Gaussian distribution

\todo{doplnit}
\todo{unify normal/gaussian naming}
\end{thm}

\begin{thm}[\citep{mitzenmacher2017probability}]
Let $X$ and $Y$ be independent random variables with distributions $\gN(\mu_1, \sigma_1^2)$ and $\gN(\mu_2, \sigma_2^2)$, respectively. Then $X + Y$ is distributed according to the normal distribution $\gN(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$.
\end{thm}
\begin{proof}
The moment generating function of a sum of independent random variables is the product of their moment generating functions (Theorem \ref{thm:sum-independent-gaussian}). Thus,

\begin{equation}
M_{X + Y}(t) = M_X(t) M_Y(t) = e^{t^2 \sigma_1^2 / 2 + \mu_1 t} e^{t^2 \sigma_2^2 / 2 + \mu_2 t} = e^{t^2 (\sigma_1^2 + \sigma_1^2) / 2 + (\mu_1 + \mu_2) t}
\end{equation}

The rightmost expression is the moment generating function of a normal distribution. Theorem \ref{thm:sum-independent-gaussian} implies that $X + Y$ has a normal distribution with the corresponding parameters. \todo{opsano z \citep{mitzenmacher2017probability}}
\end{proof}

\citep{eisenberg2008sum} \todo{hintnout to other versions of the proof? maybe show them instead?}

TODO notes:

\begin{enumerate}
\item sum of gaussians + other densities tends towards Gaussian (central limit theorem\
\item sum of gaussians is a gaussian
\item scaling a gaussian leads to a gaussian
\end{enumerate}


\begin{thm} Conditional and marginal Gaussian parameters \citep{murphy2012machine} \todo{popsat vetu poradne}

\begin{align}
p(x_1, x_2) &= p(x_1 | x_2) p(x_2) \\
&= \gN(x_1 | \vmu_{1|2}, \ms{1|2}) \gN(x_2 | \vmu_2, \ms{22})
\end{align}

the parameters are \todo{zkontrolovat kde je $\mu$ a chybi tam vector}

\begin{align}
\vmu_{1|2} &= \vmu_1 - \ms{12} \ms{22}^{-1} (\vx_2 - \vmu_2) \\
\ms{1|2} &= \mSigma / \ms{22} = \ms{11} - \ms{12} \ms{22}^{-1} \ms{21}
\end{align}
\end{thm}

\begin{proof}

To make the equations more readable, we define 

\begin{align}
\vy_1 &= \vx_1 - \vmu_1 \\
\vy_2 &= \vx_2 - \vmu_2
\end{align}

\todo{chybi tecka?}

We then simply take the block definition of a multivariate Gaussian and multiply everything out

\begin{align}
E &= \exp \left\lbrace -\frac{1}{2}
\begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix}^T
\partsigma
\begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix} \right\rbrace \\
%
&= \exp \left\lbrace -\frac{1}{2}
\begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix}^T
\begin{bmatrix} \mI & \mZero \\ -\mSigma_{22}^{-1} \mSigma_{21} & \mI \end{bmatrix}
\begin{bmatrix} (\mSigma/\mSigma_{22})^{-1} & \mZero \\ \mZero & \mSigma_{22}^{-1} \end{bmatrix}
\begin{bmatrix} \mI & -\mSigma_{12} \mSigma_{22}^{-1} & \\ \mZero & \mI \end{bmatrix}
\begin{bmatrix} \vy_1 \\ \vy_2 \end{bmatrix} \right\rbrace \\
%
&= \exp \left\lbrace -\frac{1}{2}
\begin{bmatrix} \vy_1^T - \vy_2^T (\mSigma_{22}^{-1} \mSigma_{21}) \\
\vy_2
\end{bmatrix}^T
\begin{bmatrix} (\mSigma/\mSigma_{22})^{-1} & \mZero \\ \mZero & \mSigma_{22}^{-1} \end{bmatrix} \begin{bmatrix} \vy_1 -\mSigma_{12} \mSigma_{22}^{-1} (\vy_2) \\ \vy_2 \end{bmatrix} \right\rbrace \\
%
&= \exp \left\lbrace -\frac{1}{2}
\begin{bmatrix} (\vy_1^T - \vy_2^T \mSigma_{22}^{-1} \mSigma_{21}) (\mSigma/\mSigma_{22})^{-1} \\
\vy_2^T \mSigma_{22}^{-1}
\end{bmatrix}^T
\begin{bmatrix} \vy_1 -\mSigma_{12} \mSigma_{22}^{-1} (\vy_2) \\ \vy_2 \end{bmatrix}
\right\rbrace \\
%
&= \exp \left\lbrace -\frac{1}{2}
(\vy_1^T - \vy_2^T \mSigma_{22}^{-1} \mSigma_{21}) (\mSigma/\mSigma_{22})^{-1} (\vy_1 -\mSigma_{12} \mSigma_{22}^{-1} \vy_2)
\right\rbrace \times \\
& \qquad\qquad \times \exp \left\lbrace -\frac{1}{2} \vy_2^T \mSigma_{22}^{-1} \vy_2 \right\rbrace \nonumber
\end{align}

We can immediately see that the second term is a quadratic form in $\vx_2$ and corresponds to $\gN(\vx_2 | \vmu_2, \mSigma_{22})$. Let us now consider the first term in isolation and move the terms around a little bit. We also make use of the fact that because $\mSigma_{22}$ is a positive-definite \todo{check this and maybe show a small proof?} matrix, its inverse is also symmetric, so $\mSigma^{-1^T}_{22} = \mSigma^{-1}_{22}$. We also know that $\mSigma^T_{12} = \mSigma_{21}$.

\begin{align}
E_{1|2} &= \exp \left\lbrace -\frac{1}{2}
(\vy_1^T - \vy_2^T \mSigma_{22}^{-1} \mSigma_{21}) (\mSigma/\mSigma_{22})^{-1} (\vy_1 -\mSigma_{12} \mSigma_{22}^{-1} \vy_2) \right\rbrace \\
&= \exp \left\lbrace -\frac{1}{2}
(\vy_1 - \mSigma_{12}\mSigma_{22}^{-1} \vy_2)^T (\mSigma/\mSigma_{22})^{-1} (\vy_1 -\mSigma_{12} \mSigma_{22}^{-1} \vy_2) \right\rbrace \\
&= \exp \left\lbrace -\frac{1}{2}
(\vx_1 - \vmu_1 - \mSigma_{12}\mSigma_{22}^{-1} (\vx_2 - \vmu_2)^T (\mSigma/\mSigma_{22})^{-1} (\vx_1 - \vmu_1 -\mSigma_{12} \mSigma_{22}^{-1} (\vx_2 - \vmu_2)) \right\rbrace \label{eq:mvn-quadratic-form-12}
\end{align}

In \eqref{eq:mvn-quadratic-form-12} we again see a Gaussian density with

\begin{align}
\vmu_{1|2} &= \vmu_1 - \mSigma_{12}\mSigma_{22}^{-1} (\vx_2 - \vmu_2) \\
\mSigma_{1|2} &= (\mSigma/\mSigma_{22})^{-1} =  \ms{11} - \ms{12} \ms{22}^{-1} \ms{21}
\end{align}
\end{proof}


\begin{tcolorbox}
    \begin{defn}
    \newterm{Affine transformation}:
    
    \begin{equation}
        f(\vx) = \mA \vx + \vb
    \end{equation}
    \end{defn}
    
    \begin{thm}
    \newterm{Affine property}: Any affine transformation of a Gaussian is a Gaussian. That is, if $\rX \sim \gN(\vmu, \mSigma)$, then $\mA \rX + \vb \sim \gN(\mA \vmu + \vb, \mA \mSigma \mA^T)$ (for any $\vmu \in \mR^n, \mSigma \in \mR^{n \times n}$ positive semi-definite, and any $\mA \in \mR^{m \times n}, \vb \in \mR^m$.
    
    (Constructing) $\rX_1, \dots, \rX_n \sim \gN(0, 1)$ independent $\implies \rX \sim \gN(0, \mI)$, which also implies $\mA \rX + \vmu \sim \gN(\vmu, \mSigma)$ where $\mSigma = \mA \mA^T$ (for any $\vmu \in \mR^n, \mA \in \mR^{m \times n}$).
    \end{thm}
    
    \begin{thm}
    \newterm{Sphering}: If $\mSigma$ is positive-definite, then:
    
    \begin{equation}
        \mY \sim \gN(\vmu, \mSigma) \implies \mA^{-1} (\mY - \vmu) \sim \gN(0, \mI) \text{where} \mSigma = \mA \mA^T
    \end{equation}
    
    The random variable $\mA^-1 (\mY - \vmu)$ is then a unit sphere in $n$-dimensional space.
    \end{thm}
    
    \section{Geometric intuition}
    
    Let $\mX \sim \gN(0, \mI)$. Let $\mSigma$ be a covariance matrix, and $\vmu \in \mR^n$. Covariance matrix is always positive semi-definite, which means its eigenvalues are non-negative \todo{better text}. Because $\mSigma$ is a real, symmetric, positive semi-definite matrix it can be written as $\mSigma = \mQ \mLambda \mQ^T$ where $\mQ$ is an orthogonal matrix of eigenvectors, and $\mLambda$ is a diagonal matrix of eigenvalues \todo{proof in appendix}.
    
    We can then do a simple algebraic manipulation to get the square root of $\mSigma = \mA \mA^T$. \todo{equivalent to cholesky decomp?}
    
    \begin{equation}
        \mSigma = \mQ \mLambda \mQ^T = \mQ \mLambda^{1/2} \mLambda^{1/2} \mQ = (\mQ \mLambda^{1/2}) (\mQ \mLambda^{1/2})^T = \mA \mA^T
    \end{equation}
    
    Now we can do $\mY = \mA \mX + \vmu \implies \mY \sim \gN(\vmu, \mSigma)$.
    
    Draw level sets of $\mX \sim \gN(0, \mI)$, $\mLambda^{1/2} \mX \sim \gN(0, \mLambda)$, $\mQ \mLambda^{1/2} \mX \sim \gN(0, \mSigma)$. $\mQ$ contains the eigenvectors of $\mSigma$, and multiplication by $\mQ$ rotates (every orthogonal matrix is equivalent to a rotation \todo{proof appendix}) the variable so that the axes are aligned to the eigenvectors. In other words, $\mQ$ is a change of basis from the eigenvector basis to the canonical basis, causing the rotation. $\mQ \mLambda^{1/2} \mX + \vmu \sim \gN(\vmu, \mSigma)$, in which case $\vmu$ simply shifts the whole coordinate space by $\vmu$, which in the case of a Gaussian only affects the mean \todo{proof}.
    
    In summary, the eigenvalues in $\mLambda$ scale, $\mQ$ rotates in the direction of the eigenvectors, and $\vmu$ shifts the origin.
    
    \begin{equation}
        (\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu) = (\vx - \vmu)^T \mQ \mLambda^{-1} \mQ (\vx - \vmu)
    \end{equation}
    
    \begin{equation}
        \mLambda^{-1} = \begin{bmatrix}
            \lambda_1^{-1} & & \\
            & \ddots & \\
            & & \lambda_n^{^-1}
        \end{bmatrix}
    \end{equation}
    
    TODO: Note the order of eigenvalues: large eigenvalue of $\mSigma$ is the direction with the ???smallest??? variance?
\end{tcolorbox}

\begin{tcolorbox}
    \section{Marginal and Conditional distribution}
    
    If $\mX = (\mX_1, \mX_2) \in \sR^2$ are jointly Gaussian, then $\mX_1, \mX_2$ individually are also Gaussian. \todo{write it as a theorem}
    
    \begin{proof}
    By the affine property, $\mA \mX + \vb$ is always a Gaussian. Let us set $\mA = (\mI 0), \vb = 0$. We get $\gN(\mA \vmu + \vb, \mA \mSigma \mA^T)$ and see that both $\mX_1$ and $\mX_2$ are Gaussian.
    
    More generally $\mX \sim \gN(\vmu, \mSigma), \va = (1, \dots, k), b = (k+1, \dots, n), 1 \leq k \leq n$.
    
    \begin{align}
        \mX &= \begin{bmatrix} \mX_a \\ \mX_b \end{bmatrix},
        \mX_a = \begin{bmatrix} \mX_1 \\ \vdots \\ \mX_k \end{bmatrix},
        \mX_b = \begin{bmatrix} \mX_{k+1} \\ \vdots \\ \mX_n \end{bmatrix} \\
        \vmu &= \begin{bmatrix} \vmu_a \\ \vmu_b \end{bmatrix} \\
        \mSigma &= \begin{bmatrix}
            \mSigma_{aa} & \mSigma_{ab} \\
            \mSigma_{ba} & \mSigma_{bb}
        \end{bmatrix}
    \end{align}
    
    We then show that:
    
    \begin{equation}
    \mX_a \sim \gN(\vmu_a, \mSigma_{aa})
    \end{equation}
    
    This is true because can use the affine property with a projection matrix on the first $k$ dimensions.
    
    \begin{equation}
        \mA = \begin{bmatrix} \mI_k 0 \end{bmatrix}, \quad \mA \in \sR^{k \times n}
    \end{equation}
    
    By construction, $\mA \mX = \mX_a \sim \gN(\mA \vmu, \mA \mSigma \mA^T)$, where $\mA \vmu = \mu_a$, and $\mA \mSigma \mA^T = \mSigma_{aa}$, and thus $\mX_a \sim \gN(\vmu_a, \mSigma_{aa})$.
    
    \end{proof}
    
    \begin{thm}
    Same as before, we have $\mX_1, \mX_2$ which are jointly Gaussian, and we'll derive the conditional distribution $p(\rx_1 | \rx_2)$.
    \end{thm}
    
    \begin{align}
        \mX_1 | \mX_2 & \sim \gN(m, D) \\
        m &= \vmu_a + \ms{ab} \ms{bb}^{-1} ( \rx_b - \vmu_b) \\
        D &= \ms{aa} - \ms{ab} \ms{bb}^{-1} \ms{ba}
    \end{align}
    
    Ma ($\vmu_a$) gets in a cab ($\ms{ab}$), sees it is a bb gun ($\ms{bb}$), inverts the bb gun and turns it back on the cabby guy ($\ms{bb}^{-1}$), xses the cabby ($\rx_b)$, and then ma gets her money back ($- \vmu_b)$.
    
    \begin{equation}
        \det \begin{bmatrix}
            \mSigma_{aa} & \mSigma_{ab} \\
            \mSigma_{ba} & \mSigma_{bb}
        \end{bmatrix} = \ms{aa} \ms{bb} - \ms{ab} \ms{ba}
    \end{equation}
    
    and then we try to move $\ms{bb}$ in the right term, it goes in the middle, and we get $\ms{aa} - \ms{ab} \ms{bb}^{-1} \ms{ba}$.

\end{tcolorbox}

\begin{tcolorbox}

    \section{Sum of independent Gaussians}
    
    If $\mX \in \sR^n \sim \gN(\vmu_x, \mSigma_x)$ and $\mY \in \sR^n \sim \gN(\vmu_y, \mSigma_y)$ are independent, then
    
    \begin{equation}
        \mX + \mY \sim \gN(\vmu_x + \vmu_y, \mSigma_x + \mSigma_y)
    \end{equation}
    
    \todo{proper proof}
    \begin{proof}
    Case $n=1$, $\mX, \mY \in \sR$ independent Gaussian
    
    General case:
    \end{proof}
    
    \begin{thm}
    $cov(\mX + \mY) = cov(\mX) + cov(\mY)$ for independent $\mX$ and $\mY$.
    \end{thm}
    
    \begin{proof}
    \begin{align}
        cov(\mX + \mY) &= E((\mX + \mY) (\mX + \mY)^T) - E(\mX + \mY)E(\mX + \mY)^T \\
        &= E\mX\mX^T + E\mX\mY^T + E\mY\mX^T + E\mY\mY^T \\
          &\quad - (E\mX E\mX^T + E\mX E\mY^T + E\mY E\mX^T + E \mY E\mY^T)
    \end{align}
    
    Because $\mX, \mY$ are independent, $E \mX \mY^T = E \mX E \mY^T$, and we get:
    
    \begin{align}
        &= E\mX\mX^T + E\mX \mY^T + E\mY\mX^T + E\mY\mY^T \\
          &\quad - (E\mX E\mX^T + E\mX E\mY^T + E\mY E\mX^T + \mY E\mY^T) \\
        &= E\mX\mX^T - E\mX E\mX^T + E\mY \mY^T - E\mY E\mY^T \\
        &= cov(\mX) + cov(\mY)
    \end{align}
    \end{proof}
\end{tcolorbox}


\section{Sampling from a Gaussian}

Before we can get to sampling, let us show how linear transformations affect the covariance of a random variable. Let $cov[\mX] = \mSigma$, it follows that:

\begin{align}
cov[\mA \mX] &= E[(\mA \mX - E[\mA \mX])(\mA \mX - E[\mA \mX])^T] \\
&= E[(\mA \mX - \mA E[\mX])(\mA \mX - \mA E[\mX])^T] \\
&= E[\mA (\mX - E[\mX])(\mX - E[\mX])^T \mA^T] \\
&= \mA E[(\mX - E[\mX])(\mX - E[\mX])^T] \mA^T \\
&= \mA cov[\mX] \mA^T \\
&= \mA \Sigma \mA^T
\end{align}

Now using a Cholesky decomposition \todo{ref}, we get $\mA = \mL \mL^T$, from which it simply follows that:

\begin{align}
\mX \sim N(0, \mI) \\
\mL \mX \sim N(0, \mL \mI \mL^T) = N(0, \mL \mL^T) = N(0, \mSigma)
\end{align}

This allows us to take sample from a Gaussian with identity covariance and transform it to one with arbitrary covariance matrix. To draw samples from an isotropic Gaussian we can use inverse transform sampling \todo{ref}, or the Box-Muller transform \todo{ref}.

\section{Linear Gaussian transform}

\begin{align}
P(\vx \in \mM) = \int_\mM \frac{1}{(2 \pi)^{D/2} |\mSigma|^{1/2}|} exp \left(-\frac{1}{2} (\vx - \vmu)^T \mSigma^{-1} (\vx - \vmu) \right) d \vx \\
P(\vx \in \mA^{-1} \mM) = \int_{\mA^{-1} \mM} \frac{1}{(2 \pi)^{D/2} |\mSigma|^{1/2}} \exp \left(-\frac{1}{2} (\vu - \vmu)^T \mSigma^{-1} (\vu - \vmu) \right) d \vu \\
\vu = \mA \vx
\end{align}


        